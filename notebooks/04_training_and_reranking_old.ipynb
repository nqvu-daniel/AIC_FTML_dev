{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIC Video Retrieval System - Training & Reranking\n",
    "\n",
    "This notebook handles training reranking models to improve search result quality.\n",
    "It includes both cross-encoder and gradient boosting approaches for reranking.\n",
    "\n",
    "## Features\n",
    "- üìä Training data preparation and validation\n",
    "- ü§ñ Cross-encoder reranker training\n",
    "- üå≤ Gradient boosting reranker training\n",
    "- üìà Training progress monitoring\n",
    "- üß™ Model evaluation and comparison\n",
    "- üíæ Model checkpoint management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Set up paths (assuming setup notebook was run)\n",
    "REPO_NAME = \"AIC_FTML_dev\"\n",
    "if Path(f\"/content/{REPO_NAME}\").exists():\n",
    "    REPO_DIR = Path(f\"/content/{REPO_NAME}\")\n",
    "else:\n",
    "    REPO_DIR = Path.cwd()\n",
    "    while REPO_DIR.name != REPO_NAME and REPO_DIR.parent != REPO_DIR:\n",
    "        REPO_DIR = REPO_DIR.parent\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "sys.path.insert(0, str(REPO_DIR / \"src\"))\n",
    "\n",
    "print(f\"Working from: {REPO_DIR}\")\n",
    "\n",
    "# Import ML libraries\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "    from sentence_transformers.cross_encoder import CrossEncoder\n",
    "    from sentence_transformers.cross_encoder.evaluation import CERerankingEvaluator\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "    print(\"‚úÖ ML libraries loaded successfully\")\nexcept ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Some ML libraries missing: {e}\")\n",
    "    print(\"Installing missing dependencies...\")\n",
    "    !pip install sentence-transformers scikit-learn xgboost\n",
    "\n",
    "# Import project modules\n",
    "import config\n",
    "from src.models.clip_encoder import CLIPEncoder\n",
    "from src.pipeline.query_pipeline import QueryProcessingPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"=== Training Data Preparation ===\")\n",
    "\n",
    "# Check for training data\n",
    "training_files = [\n",
    "    Path(\"data/train.jsonl\"),\n",
    "    Path(\"data/training_data.jsonl\"),\n",
    "    Path(\"train.jsonl\")\n",
    "]\n",
    "\n",
    "training_file = None\n",
    "for file_path in training_files:\n",
    "    if file_path.exists():\n",
    "        training_file = file_path\n",
    "        break\n",
    "\n",
    "if not training_file:\n",
    "    print(\"‚ùå No training data found. Creating sample training data...\")\n",
    "    \n",
    "    # Try to create training data using the utility\n",
    "    try:\n",
    "        from utils.create_training_data import create_training_data_from_metadata\n",
    "        \n",
    "        success = create_training_data_from_metadata(\n",
    "            dataset_root=\"./data\",\n",
    "            output_file=\"data/train.jsonl\",\n",
    "            num_examples=100\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            training_file = Path(\"data/train.jsonl\")\n",
    "            print(\"‚úÖ Training data created\")\n",
    "        else:\n",
    "            print(\"‚ùå Could not create training data\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating training data: {e}\")\nelse:\n",
    "    print(f\"‚úÖ Found training data: {training_file}\")\n",
    "\n",
    "# Load and analyze training data\n",
    "training_data = []\nif training_file and training_file.exists():\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                training_data.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print(f\"Loaded {len(training_data)} training examples\")\n",
    "    \n",
    "    if training_data:\n",
    "        # Analyze training data\n",
    "        total_positives = sum(len(item.get('positives', [])) for item in training_data)\n",
    "        unique_queries = len(set(item['query'] for item in training_data))\n",
    "        \n",
    "        print(f\"Training data statistics:\")\n",
    "        print(f\"  Total queries: {len(training_data)}\")\n",
    "        print(f\"  Unique queries: {unique_queries}\")\n",
    "        print(f\"  Total positive examples: {total_positives}\")\n",
    "        print(f\"  Avg positives per query: {total_positives/len(training_data):.1f}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nüìã Sample training examples:\")\n",
    "        for i, item in enumerate(training_data[:3]):\n",
    "            print(f\"{i+1}. Query: '{item['query']}'\")\n",
    "            print(f\"   Positives: {len(item.get('positives', []))} frames\")\n",
    "            if item.get('positives'):\n",
    "                sample_pos = item['positives'][0]\n",
    "                print(f\"   Sample: {sample_pos['video_id']} frame {sample_pos['frame_idx']}\")\n",
    "else:\n",
    "    print(\"‚ùå No training data available. Please run data processing notebook first.\")\n",
    "    training_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training Pairs for Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training pairs for reranking\n",
    "print(\"=== Creating Training Pairs ===\")\n",
    "\n",
    "def create_reranking_pairs(training_data, negative_sampling_ratio=2):\n",
    "    \"\"\"Create query-document pairs with relevance scores for reranking training\"\"\"\n",
    "    \n",
    "    if not training_data:\n",
    "        return [], []\n",
    "    \n",
    "    # Load index metadata to get negative samples\n",
    "    ARTIFACT_DIR = Path(config.ARTIFACT_DIR)\n",
    "    metadata_file = ARTIFACT_DIR / \"index_metadata.parquet\"\n",
    "    \n",
    "    if not metadata_file.exists():\n",
    "        print(\"‚ùå Index metadata not found\")\n",
    "        return [], []\n",
    "    \n",
    "    metadata_df = pd.read_parquet(metadata_file)\n",
    "    all_frames = [(row['video_id'], row['frame_idx']) for _, row in metadata_df.iterrows()]\n",
    "    \n",
    "    pairs = []  # (query, video_id_frame_idx)\n",
    "    labels = []  # relevance scores (1 for positive, 0 for negative)\n",
    "    \n",
    "    for item in tqdm(training_data, desc=\"Creating pairs\"):\n",
    "        query = item['query']\n",
    "        positives = item.get('positives', [])\n",
    "        \n",
    "        if not positives:\n",
    "            continue\n",
    "        \n",
    "        # Add positive pairs\n",
    "        for pos in positives:\n",
    "            doc_id = f\"{pos['video_id']}_frame_{pos['frame_idx']}\"\n",
    "            pairs.append((query, doc_id))\n",
    "            labels.append(1)  # Positive label\n",
    "        \n",
    "        # Sample negative examples\n",
    "        positive_frames = {(pos['video_id'], pos['frame_idx']) for pos in positives}\n",
    "        negative_candidates = [frame for frame in all_frames if frame not in positive_frames]\n",
    "        \n",
    "        if negative_candidates:\n",
    "            num_negatives = min(len(positives) * negative_sampling_ratio, len(negative_candidates))\n",
    "            negative_samples = np.random.choice(\n",
    "                len(negative_candidates), \n",
    "                size=num_negatives, \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            for neg_idx in negative_samples:\n",
    "                neg_frame = negative_candidates[neg_idx]\n",
    "                doc_id = f\"{neg_frame[0]}_frame_{neg_frame[1]}\"\n",
    "                pairs.append((query, doc_id))\n",
    "                labels.append(0)  # Negative label\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "# Create training pairs\n",
    "if training_data:\n",
    "    print(\"Creating query-document pairs...\")\n",
    "    train_pairs, train_labels = create_reranking_pairs(training_data)\n",
    "    \n",
    "    if train_pairs:\n",
    "        print(f\"‚úÖ Created {len(train_pairs)} training pairs\")\n",
    "        print(f\"   Positive pairs: {sum(train_labels)}\")\n",
    "        print(f\"   Negative pairs: {len(train_labels) - sum(train_labels)}\")\n",
    "        print(f\"   Positive ratio: {sum(train_labels)/len(train_labels):.2%}\")\n",
    "        \n",
    "        # Split training data\n",
    "        train_pairs_train, train_pairs_val, train_labels_train, train_labels_val = train_test_split(\n",
    "            train_pairs, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nSplit into:\")\n",
    "        print(f\"  Training: {len(train_pairs_train)} pairs\")\n",
    "        print(f\"  Validation: {len(train_pairs_val)} pairs\")\n",
    "    else:\n",
    "        print(\"‚ùå No training pairs created\")\n",
    "        train_pairs_train = train_pairs_val = []\n",
    "        train_labels_train = train_labels_val = []\nelse:\n",
    "    print(\"‚ö†Ô∏è No training data available for pair creation\")\n",
    "    train_pairs_train = train_pairs_val = []\n",
    "    train_labels_train = train_labels_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cross-Encoder Reranker Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Cross-Encoder reranker\n",
    "print(\"=== Cross-Encoder Reranker Training ===\")\n",
    "\n",
    "TRAIN_CROSS_ENCODER = True  # Set to False to skip\n",
    "\n",
    "if TRAIN_CROSS_ENCODER and train_pairs_train:\n",
    "    try:\n",
    "        # Initialize cross-encoder\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Choose base model\n",
    "        cross_encoder_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # Lightweight model\n",
    "        \n",
    "        print(f\"Initializing cross-encoder: {cross_encoder_model}\")\n",
    "        cross_encoder = CrossEncoder(cross_encoder_model, num_labels=1)\n",
    "        \n",
    "        # Prepare training data for cross-encoder\n",
    "        train_samples = []\n",
    "        for (query, doc_id), label in zip(train_pairs_train, train_labels_train):\n",
    "            train_samples.append([query, doc_id, label])\n",
    "        \n",
    "        val_samples = []\n",
    "        for (query, doc_id), label in zip(train_pairs_val, train_labels_val):\n",
    "            val_samples.append([query, doc_id, label])\n",
    "        \n",
    "        print(f\"Prepared {len(train_samples)} training samples\")\n",
    "        print(f\"Prepared {len(val_samples)} validation samples\")\n",
    "        \n",
    "        # Training configuration\n",
    "        train_batch_size = 16\n",
    "        num_epochs = 3\n",
    "        warmup_steps = 100\n",
    "        \n",
    "        print(f\"\\nTraining configuration:\")\n",
    "        print(f\"  Batch size: {train_batch_size}\")\n",
    "        print(f\"  Epochs: {num_epochs}\")\n",
    "        print(f\"  Warmup steps: {warmup_steps}\")\n",
    "        \n",
    "        # Setup evaluator\n",
    "        evaluator = CERerankingEvaluator(val_samples, name='validation')\n",
    "        \n",
    "        # Start training\n",
    "        print(\"\\nüèãÔ∏è Starting cross-encoder training...\")\n",
    "        \n",
    "        output_path = Path(\"./artifacts/cross_encoder_reranker\")\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        cross_encoder.fit(\n",
    "            train_dataloader=train_samples,\n",
    "            evaluator=evaluator,\n",
    "            epochs=num_epochs,\n",
    "            evaluation_steps=500,\n",
    "            warmup_steps=warmup_steps,\n",
    "            output_path=str(output_path),\n",
    "            save_best_model=True,\n",
    "            optimizer_params={'lr': 2e-5},\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Cross-encoder training completed!\")\n",
    "        print(f\"Model saved to: {output_path}\")\n",
    "        \n",
    "        # Test the trained model\n",
    "        test_query = \"news anchor speaking\"\n",
    "        test_docs = [\"video1_frame_100\", \"video2_frame_200\", \"video3_frame_300\"]\n",
    "        \n",
    "        test_pairs = [[test_query, doc] for doc in test_docs]\n",
    "        scores = cross_encoder.predict(test_pairs)\n",
    "        \n",
    "        print(f\"\\nüß™ Test prediction:\")\n",
    "        print(f\"Query: '{test_query}'\")\n",
    "        for doc, score in zip(test_docs, scores):\n",
    "            print(f\"  {doc}: {score:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cross-encoder training failed: {e}\")\n",
    "        print(\"This might be due to insufficient data or memory constraints\")\nelse:\n",
    "    if not TRAIN_CROSS_ENCODER:\n",
    "        print(\"‚ö†Ô∏è Cross-encoder training skipped\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Cross-encoder training not possible - no training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Gradient Boosting Reranker Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting reranker\n",
    "print(\"=== Gradient Boosting Reranker Training ===\")\n",
    "\n",
    "TRAIN_GBM = True  # Set to False to skip\n",
    "\n",
    "def extract_features_for_gbm(query_doc_pairs, clip_encoder=None):\n",
    "    \"\"\"Extract features for gradient boosting model\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for query, doc_id in tqdm(query_doc_pairs, desc=\"Extracting features\"):\n",
    "        # Basic text features\n",
    "        feature_vector = [\n",
    "            len(query),  # Query length\n",
    "            len(query.split()),  # Query word count\n",
    "            len(doc_id),  # Document ID length\n",
    "            1 if 'frame' in doc_id else 0,  # Has 'frame' in doc ID\n",
    "            query.count(' '),  # Space count in query\n",
    "        ]\n",
    "        \n",
    "        # Text overlap features\n",
    "        query_words = set(query.lower().split())\n",
    "        doc_words = set(doc_id.lower().replace('_', ' ').split())\n",
    "        \n",
    "        if query_words:\n",
    "            overlap_ratio = len(query_words & doc_words) / len(query_words)\n",
    "        else:\n",
    "            overlap_ratio = 0\n",
    "        \n",
    "        feature_vector.extend([\n",
    "            overlap_ratio,  # Word overlap ratio\n",
    "            len(query_words & doc_words),  # Overlap count\n",
    "        ])\n",
    "        \n",
    "        # Query type features (simple heuristics)\n",
    "        feature_vector.extend([\n",
    "            1 if any(word in query.lower() for word in ['news', 'anchor', 'reporter']) else 0,\n",
    "            1 if any(word in query.lower() for word in ['speaking', 'talking', 'presenting']) else 0,\n",
    "            1 if any(word in query.lower() for word in ['studio', 'broadcast', 'television']) else 0,\n",
    "            1 if len([c for c in query if ord(c) > 127]) > 0 else 0,  # Has non-ASCII (Vietnamese)\n",
    "        ])\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "if TRAIN_GBM and train_pairs_train:\n",
    "    try:\n",
    "        print(\"Extracting features for gradient boosting...\")\n",
    "        \n",
    "        # Extract features\n",
    "        X_train = extract_features_for_gbm(train_pairs_train)\n",
    "        y_train = np.array(train_labels_train)\n",
    "        \n",
    "        X_val = extract_features_for_gbm(train_pairs_val)\n",
    "        y_val = np.array(train_labels_val)\n",
    "        \n",
    "        print(f\"‚úÖ Extracted features:\")\n",
    "        print(f\"  Training: {X_train.shape}\")\n",
    "        print(f\"  Validation: {X_val.shape}\")\n",
    "        print(f\"  Feature dimension: {X_train.shape[1]}\")\n",
    "        \n",
    "        # Train gradient boosting model\n",
    "        print(\"\\nüå≤ Training gradient boosting reranker...\")\n",
    "        \n",
    "        gbm_model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        gbm_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        train_pred = gbm_model.predict(X_train)\n",
    "        val_pred = gbm_model.predict(X_val)\n",
    "        \n",
    "        train_mse = mean_squared_error(y_train, train_pred)\n",
    "        val_mse = mean_squared_error(y_val, val_pred)\n",
    "        \n",
    "        print(f\"\\nüìä GBM Training Results:\")\n",
    "        print(f\"  Training MSE: {train_mse:.4f}\")\n",
    "        print(f\"  Validation MSE: {val_mse:.4f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_names = [\n",
    "            'query_length', 'query_words', 'doc_length', 'has_frame', 'space_count',\n",
    "            'overlap_ratio', 'overlap_count', 'is_news', 'is_speaking', 'is_studio', 'is_vietnamese'\n",
    "        ]\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': gbm_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nüéØ Feature Importance:\")\n",
    "        display(importance_df)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = Path(\"./artifacts/gbm_reranker.pkl\")\n",
    "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': gbm_model,\n",
    "                'feature_names': feature_names,\n",
    "                'training_stats': {\n",
    "                    'train_mse': train_mse,\n",
    "                    'val_mse': val_mse,\n",
    "                    'n_samples': len(X_train)\n",
    "                }\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"‚úÖ GBM model saved to: {model_path}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        test_pairs = [\n",
    "            (\"news anchor speaking\", \"video1_frame_100\"),\n",
    "            (\"person talking\", \"video2_frame_200\")\n",
    "        ]\n",
    "        \n",
    "        test_features = extract_features_for_gbm(test_pairs)\n",
    "        test_scores = gbm_model.predict(test_features)\n",
    "        \n",
    "        print(f\"\\nüß™ GBM Test predictions:\")\n",
    "        for (query, doc), score in zip(test_pairs, test_scores):\n",
    "            print(f\"  '{query}' + '{doc}': {score:.4f}\")\n",
    "            \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importance_df, x='importance', y='feature')\n",
    "        plt.title('Feature Importance in GBM Reranker')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GBM training failed: {e}\")\nelse:\n",
    "    if not TRAIN_GBM:\n",
    "        print(\"‚ö†Ô∏è GBM training skipped\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GBM training not possible - no training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare reranking models\n",
    "print(\"=== Model Evaluation and Comparison ===\")\n",
    "\n",
    "def evaluate_reranker_performance():\n",
    "    \"\"\"Evaluate trained rerankers on validation data\"\"\"\n",
    "    \n",
    "    if not train_pairs_val:\n",
    "        print(\"‚ùå No validation data available\")\n",
    "        return\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Load models if available\n",
    "    cross_encoder_path = Path(\"./artifacts/cross_encoder_reranker\")\n",
    "    gbm_model_path = Path(\"./artifacts/gbm_reranker.pkl\")\n",
    "    \n",
    "    print(f\"Evaluating on {len(train_pairs_val)} validation pairs...\")\n",
    "    \n",
    "    # Baseline: random ranking\n",
    "    baseline_scores = np.random.rand(len(train_pairs_val))\n",
    "    baseline_mse = mean_squared_error(train_labels_val, baseline_scores)\n",
    "    results['Random Baseline'] = {'mse': baseline_mse, 'scores': baseline_scores}\n",
    "    \n",
    "    # Cross-encoder evaluation\n",
    "    if cross_encoder_path.exists():\n",
    "        try:\n",
    "            print(\"Evaluating cross-encoder...\")\n",
    "            cross_encoder = CrossEncoder(str(cross_encoder_path))\n",
    "            \n",
    "            ce_pairs = [[query, doc] for query, doc in train_pairs_val]\n",
    "            ce_scores = cross_encoder.predict(ce_pairs)\n",
    "            ce_mse = mean_squared_error(train_labels_val, ce_scores)\n",
    "            \n",
    "            results['Cross-Encoder'] = {'mse': ce_mse, 'scores': ce_scores}\n",
    "            print(f\"  Cross-encoder MSE: {ce_mse:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Cross-encoder evaluation failed: {e}\")\n",
    "    \n",
    "    # GBM evaluation\n",
    "    if gbm_model_path.exists():\n",
    "        try:\n",
    "            print(\"Evaluating GBM reranker...\")\n",
    "            with open(gbm_model_path, 'rb') as f:\n",
    "                gbm_data = pickle.load(f)\n",
    "                gbm_model = gbm_data['model']\n",
    "            \n",
    "            gbm_features = extract_features_for_gbm(train_pairs_val)\n",
    "            gbm_scores = gbm_model.predict(gbm_features)\n",
    "            gbm_mse = mean_squared_error(train_labels_val, gbm_scores)\n",
    "            \n",
    "            results['GBM Reranker'] = {'mse': gbm_mse, 'scores': gbm_scores}\n",
    "            print(f\"  GBM MSE: {gbm_mse:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  GBM evaluation failed: {e}\")\n",
    "    \n",
    "    # Compare results\n",
    "    if len(results) > 1:\n",
    "        print(\"\\nüìä Model Comparison:\")\n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name, model_results in results.items():\n",
    "            mse = model_results['mse']\n",
    "            scores = model_results['scores']\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            correlation = np.corrcoef(train_labels_val, scores)[0, 1] if len(scores) > 1 else 0\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'MSE': mse,\n",
    "                'Correlation': correlation,\n",
    "                'Score Mean': np.mean(scores),\n",
    "                'Score Std': np.std(scores)\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data).sort_values('MSE')\n",
    "        display(comparison_df.round(4))\n",
    "        \n",
    "        # Visualize score distributions\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Score distributions\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for model_name, model_results in results.items():\n",
    "            plt.hist(model_results['scores'], bins=20, alpha=0.7, label=model_name)\n",
    "        plt.xlabel('Predicted Scores')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Score Distributions')\n",
    "        plt.legend()\n",
    "        \n",
    "        # MSE comparison\n",
    "        plt.subplot(1, 2, 2)\n",
    "        mse_values = [results[name]['mse'] for name in comparison_df['Model']]\n",
    "        plt.bar(comparison_df['Model'], mse_values)\n",
    "        plt.ylabel('MSE (lower is better)')\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find best model\n",
    "        best_model = comparison_df.iloc[0]['Model']\n",
    "        print(f\"\\nüèÜ Best performing model: {best_model}\")\n",
    "        print(f\"   MSE: {comparison_df.iloc[0]['MSE']:.4f}\")\n",
    "        print(f\"   Correlation: {comparison_df.iloc[0]['Correlation']:.4f}\")\n",
    "        \n",
    "        return comparison_df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not enough models to compare\")\n",
    "        return None\n",
    "\n",
    "# Run evaluation\nif train_pairs_val:\n",
    "    evaluation_results = evaluate_reranker_performance()\nelse:\n",
    "    print(\"‚ö†Ô∏è No validation data available for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Rerankers with Live Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rerankers with actual search queries\n",
    "print(\"=== Live Search Testing with Rerankers ===\")\n",
    "\n",
    "def test_rerankers_on_search(query, k=20):\n",
    "    \"\"\"Test rerankers by comparing search results with and without reranking\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize basic search pipeline\n",
    "        ARTIFACT_DIR = Path(config.ARTIFACT_DIR)\n",
    "        query_pipeline = QueryProcessingPipeline(\n",
    "            artifact_dir=ARTIFACT_DIR,\n",
    "            enable_reranking=False  # Start without reranking\n",
    "        )\n",
    "        \n",
    "        # Get baseline results\n",
    "        print(f\"üîç Searching for: '{query}'\")\n",
    "        baseline_results = query_pipeline.search(query, k=k*2)  # Get more for reranking\n",
    "        \n",
    "        if not baseline_results:\n",
    "            print(\"‚ùå No search results found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Got {len(baseline_results)} baseline results\")\n",
    "        \n",
    "        # Test cross-encoder reranking\n",
    "        cross_encoder_path = Path(\"./artifacts/cross_encoder_reranker\")\n",
    "        if cross_encoder_path.exists():\n",
    "            try:\n",
    "                print(\"\\nüß† Testing cross-encoder reranking...\")\n",
    "                cross_encoder = CrossEncoder(str(cross_encoder_path))\n",
    "                \n",
    "                # Prepare pairs for reranking\n",
    "                ce_pairs = []\n",
    "                for result in baseline_results:\n",
    "                    doc_id = f\"{result.video_id}_frame_{result.frame_idx}\"\n",
    "                    ce_pairs.append([query, doc_id])\n",
    "                \n",
    "                # Get reranking scores\n",
    "                rerank_scores = cross_encoder.predict(ce_pairs)\n",
    "                \n",
    "                # Sort by rerank scores\n",
    "                scored_results = list(zip(baseline_results, rerank_scores))\n",
    "                scored_results.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                ce_reranked = [result for result, _ in scored_results[:k]]\n",
    "                \n",
    "                print(f\"‚úÖ Cross-encoder reranking completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Cross-encoder reranking failed: {e}\")\n",
    "                ce_reranked = None\n",
    "        else:\n",
    "            ce_reranked = None\n",
    "            print(\"‚ö†Ô∏è Cross-encoder model not found\")\n",
    "        \n",
    "        # Test GBM reranking\n",
    "        gbm_model_path = Path(\"./artifacts/gbm_reranker.pkl\")\n",
    "        if gbm_model_path.exists():\n",
    "            try:\n",
    "                print(\"\\nüå≤ Testing GBM reranking...\")\n",
    "                with open(gbm_model_path, 'rb') as f:\n",
    "                    gbm_data = pickle.load(f)\n",
    "                    gbm_model = gbm_data['model']\n",
    "                \n",
    "                # Prepare features for GBM\n",
    "                gbm_pairs = []\n",
    "                for result in baseline_results:\n",
    "                    doc_id = f\"{result.video_id}_frame_{result.frame_idx}\"\n",
    "                    gbm_pairs.append((query, doc_id))\n",
    "                \n",
    "                gbm_features = extract_features_for_gbm(gbm_pairs)\n",
    "                gbm_scores = gbm_model.predict(gbm_features)\n",
    "                \n",
    "                # Sort by GBM scores\n",
    "                scored_results = list(zip(baseline_results, gbm_scores))\n",
    "                scored_results.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                gbm_reranked = [result for result, _ in scored_results[:k]]\n",
    "                \n",
    "                print(f\"‚úÖ GBM reranking completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå GBM reranking failed: {e}\")\n",
    "                gbm_reranked = None\n",
    "        else:\n",
    "            gbm_reranked = None\n",
    "            print(\"‚ö†Ô∏è GBM model not found\")\n",
    "        \n",
    "        # Compare results\n",
    "        print(f\"\\nüìä Results Comparison (Top 10):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for i in range(min(10, len(baseline_results))):\n",
    "            row = {'Rank': i+1}\n",
    "            \n",
    "            # Baseline\n",
    "            baseline_result = baseline_results[i] if i < len(baseline_results) else None\n",
    "            if baseline_result:\n",
    "                row['Baseline'] = f\"{baseline_result.video_id}_f{baseline_result.frame_idx}\"\n",
    "                row['Baseline_Score'] = f\"{baseline_result.score:.3f}\"\n",
    "            \n",
    "            # Cross-encoder\n",
    "            if ce_reranked and i < len(ce_reranked):\n",
    "                ce_result = ce_reranked[i]\n",
    "                row['Cross-Encoder'] = f\"{ce_result.video_id}_f{ce_result.frame_idx}\"\n",
    "                row['CE_Score'] = f\"{ce_result.score:.3f}\"\n",
    "            \n",
    "            # GBM\n",
    "            if gbm_reranked and i < len(gbm_reranked):\n",
    "                gbm_result = gbm_reranked[i]\n",
    "                row['GBM'] = f\"{gbm_result.video_id}_f{gbm_result.frame_idx}\"\n",
    "                row['GBM_Score'] = f\"{gbm_result.score:.3f}\"\n",
    "            \n",
    "            comparison_data.append(row)\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        display(comparison_df)\n",
    "        \n",
    "        # Calculate ranking changes\n",
    "        if ce_reranked or gbm_reranked:\n",
    "            baseline_order = [f\"{r.video_id}_f{r.frame_idx}\" for r in baseline_results[:10]]\n",
    "            \n",
    "            if ce_reranked:\n",
    "                ce_order = [f\"{r.video_id}_f{r.frame_idx}\" for r in ce_reranked[:10]]\n",
    "                ce_changes = sum(1 for i, item in enumerate(ce_order) if i >= len(baseline_order) or item != baseline_order[i])\n",
    "                print(f\"Cross-encoder ranking changes: {ce_changes}/10\")\n",
    "            \n",
    "            if gbm_reranked:\n",
    "                gbm_order = [f\"{r.video_id}_f{r.frame_idx}\" for r in gbm_reranked[:10]]\n",
    "                gbm_changes = sum(1 for i, item in enumerate(gbm_order) if i >= len(baseline_order) or item != baseline_order[i])\n",
    "                print(f\"GBM ranking changes: {gbm_changes}/10\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Live search testing failed: {e}\")\n",
    "\n",
    "# Interactive testing widget\n",
    "test_query_widget = widgets.Text(\n",
    "    value='news anchor speaking',\n",
    "    placeholder='Enter test query...',\n",
    "    description='Test Query:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_k_widget = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=50,\n",
    "    step=5,\n",
    "    description='Results:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def run_reranker_test(query, k):\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a test query\")\n",
    "        return\n",
    "    \n",
    "    test_rerankers_on_search(query, k)\n",
    "\n",
    "# Create interactive widget\n",
    "reranker_test_widget = interactive(\n",
    "    run_reranker_test,\n",
    "    query=test_query_widget,\n",
    "    k=test_k_widget\n",
    ")\n",
    "\n",
    "print(\"üéõÔ∏è Interactive Reranker Testing:\")\ndisplay(reranker_test_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment configuration for trained models\n",
    "print(\"=== Model Deployment Configuration ===\")\n",
    "\n",
    "def create_deployment_config():\n",
    "    \"\"\"Create configuration file for deploying trained rerankers\"\"\"\n",
    "    \n",
    "    config_data = {\n",
    "        'reranking': {\n",
    "            'enabled': True,\n",
    "            'models': {},\n",
    "            'default_model': None,\n",
    "            'rerank_top_k': 1000,\n",
    "            'return_top_k': 100\n",
    "        },\n",
    "        'training_info': {\n",
    "            'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'training_examples': len(training_data) if training_data else 0,\n",
    "            'training_pairs': len(train_pairs_train) if 'train_pairs_train' in locals() else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check available models\n",
    "    cross_encoder_path = Path(\"./artifacts/cross_encoder_reranker\")\n",
    "    gbm_model_path = Path(\"./artifacts/gbm_reranker.pkl\")\n",
    "    \n",
    "    if cross_encoder_path.exists():\n",
    "        config_data['reranking']['models']['cross_encoder'] = {\n",
    "            'type': 'cross_encoder',\n",
    "            'path': str(cross_encoder_path),\n",
    "            'description': 'Neural cross-encoder reranker',\n",
    "            'batch_size': 32,\n",
    "            'requires_gpu': False\n",
    "        }\n",
    "        \n",
    "        if not config_data['reranking']['default_model']:\n",
    "            config_data['reranking']['default_model'] = 'cross_encoder'\n",
    "    \n",
    "    if gbm_model_path.exists():\n",
    "        config_data['reranking']['models']['gbm'] = {\n",
    "            'type': 'gradient_boosting',\n",
    "            'path': str(gbm_model_path),\n",
    "            'description': 'Gradient boosting reranker with handcrafted features',\n",
    "            'requires_gpu': False,\n",
    "            'fast_inference': True\n",
    "        }\n",
    "        \n",
    "        if not config_data['reranking']['default_model']:\n",
    "            config_data['reranking']['default_model'] = 'gbm'\n",
    "    \n",
    "    # Save configuration\n",
    "    config_file = Path(\"./artifacts/reranking_config.json\")\n",
    "    config_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump(config_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Deployment configuration saved to: {config_file}\")\n",
    "    print(f\"\\nConfiguration summary:\")\n",
    "    print(f\"  Available models: {len(config_data['reranking']['models'])}\")\n",
    "    print(f\"  Default model: {config_data['reranking']['default_model']}\")\n",
    "    \n",
    "    for model_name, model_config in config_data['reranking']['models'].items():\n",
    "        print(f\"  - {model_name}: {model_config['description']}\")\n",
    "    \n",
    "    return config_data\n",
    "\n",
    "# Create training summary\n",
    "def create_training_summary():\n",
    "    \"\"\"Create a summary of training results and recommendations\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'training_completed': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'data_statistics': {\n",
    "            'training_examples': len(training_data) if training_data else 0,\n",
    "            'training_pairs': len(train_pairs_train) if 'train_pairs_train' in locals() else 0,\n",
    "            'validation_pairs': len(train_pairs_val) if 'train_pairs_val' in locals() else 0\n",
    "        },\n",
    "        'models_trained': [],\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Check what was trained\n",
    "    if Path(\"./artifacts/cross_encoder_reranker\").exists():\n",
    "        summary['models_trained'].append({\n",
    "            'name': 'cross_encoder',\n",
    "            'type': 'Neural reranker',\n",
    "            'path': './artifacts/cross_encoder_reranker'\n",
    "        })\n",
    "    \n",
    "    if Path(\"./artifacts/gbm_reranker.pkl\").exists():\n",
    "        summary['models_trained'].append({\n",
    "            'name': 'gbm',\n",
    "            'type': 'Gradient boosting reranker',\n",
    "            'path': './artifacts/gbm_reranker.pkl'\n",
    "        })\n",
    "    \n",
    "    # Add recommendations\n",
    "    if len(summary['models_trained']) == 0:\n",
    "        summary['recommendations'].append(\"No models were successfully trained. Check training data availability and system resources.\")\n",
    "    elif len(summary['models_trained']) == 1:\n",
    "        summary['recommendations'].append(f\"Only {summary['models_trained'][0]['name']} was trained. Consider training additional models for comparison.\")\n",
    "    else:\n",
    "        summary['recommendations'].append(\"Multiple rerankers trained successfully. Test both in production and choose the best performing one.\")\n",
    "    \n",
    "    if summary['data_statistics']['training_examples'] < 50:\n",
    "        summary['recommendations'].append(\"Training data is limited. Consider generating more training examples for better model performance.\")\n",
    "    \n",
    "    summary['recommendations'].extend([\n",
    "        \"Test rerankers with diverse queries to ensure robust performance.\",\n",
    "        \"Monitor reranking latency in production and adjust batch sizes if needed.\",\n",
    "        \"Consider A/B testing to measure reranking impact on user satisfaction.\"\n",
    "    ])\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = Path(\"./artifacts/training_summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìã Training Summary:\")\n",
    "    print(f\"  Models trained: {len(summary['models_trained'])}\")\n",
    "    print(f\"  Training examples: {summary['data_statistics']['training_examples']}\")\n",
    "    print(f\"  Training pairs: {summary['data_statistics']['training_pairs']}\")\n",
    "    \n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    for i, rec in enumerate(summary['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Summary saved to: {summary_file}\")\n",
    "    return summary\n",
    "\n",
    "# Create configurations\n",
    "deployment_config = create_deployment_config()\n",
    "training_summary = create_training_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ TRAINING & RERANKING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use 05_end_to_end_pipeline.ipynb to test complete workflow\")\n",
    "print(\"2. Test reranking performance in production\")\n",
    "print(\"3. Consider generating more training data for better results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Model Usage\n",
    "\n",
    "This notebook has trained and evaluated reranking models to improve search quality:\n",
    "\n",
    "### Models Trained:\n",
    "1. **Cross-Encoder Reranker**: Neural model that directly scores query-document pairs\n",
    "2. **Gradient Boosting Reranker**: Feature-based model using handcrafted features\n",
    "\n",
    "### Generated Artifacts:\n",
    "- `artifacts/cross_encoder_reranker/` - Trained cross-encoder model\n",
    "- `artifacts/gbm_reranker.pkl` - Trained GBM model with features\n",
    "- `artifacts/reranking_config.json` - Deployment configuration\n",
    "- `artifacts/training_summary.json` - Training results summary\n",
    "\n",
    "### Usage in Production:\n",
    "1. Load the reranking configuration\n",
    "2. Initialize the QueryProcessingPipeline with `enable_reranking=True`\n",
    "3. The pipeline will automatically use the best available reranker\n",
    "\n",
    "### Performance Tips:\n",
    "- Cross-encoder provides better quality but is slower\n",
    "- GBM is faster and suitable for high-throughput scenarios\n",
    "- Consider A/B testing to measure real-world impact\n",
    "- Monitor inference latency and adjust batch sizes accordingly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}