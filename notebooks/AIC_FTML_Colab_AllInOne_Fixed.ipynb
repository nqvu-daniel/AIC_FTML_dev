{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üé¨ AIC FTML All-in-One Colab Pipeline\n",
    "\n",
    "**Complete intelligent video retrieval system for Google Colab**\n",
    "\n",
    "This notebook provides your full AIC FTML pipeline:\n",
    "- üì• **Dataset download** and organization from AIC CSV\n",
    "- üß† **Intelligent frame sampling** (visual complexity, scene change detection, motion analysis)\n",
    "- ü§ñ **CLIP encoding** and vector indexing with GPU acceleration\n",
    "- üîç **Hybrid search** (vector + text with RRF fusion)\n",
    "- üéØ **Training & reranking** for improved results\n",
    "- üìä **Interactive search** interface with result visualization\n",
    "\n",
    "**‚ö° Quick Start**: Update repo URL ‚Üí Run all cells ‚Üí Search your dataset!\n",
    "\n",
    "**üß† Intelligent Sampling**: Achieves 70-90% storage reduction while maintaining search quality using sophisticated computer vision algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üöÄ Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "env-setup"
   },
   "outputs": [],
   "source": [
    "# Environment detection and setup\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Detect Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"üîß Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "# Set working directory\n",
    "WORK_DIR = Path('/content') if IN_COLAB else Path.cwd()\n",
    "print(f\"üìÅ Working directory: {WORK_DIR}\")\n",
    "\n",
    "# GPU check\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üöÄ GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU available - will use CPU\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è PyTorch not available yet\")\n",
    "# Set device variable\n",
    "try:\n",
    "    import torch\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "except Exception:\n",
    "    device = 'cpu'\n",
    "print(f'Device for models: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "repo-setup"
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "REPO_URL = \"https://github.com/your-username/AIC_FTML_dev.git\"  # üîß UPDATE THIS!\n",
    "REPO_NAME = \"AIC_FTML_dev\"\n",
    "REPO_DIR = WORK_DIR / REPO_NAME\n",
    "\n",
    "if REPO_DIR.exists():\n",
    "    print(f\"‚úÖ Repository exists at {REPO_DIR}\")\n",
    "    os.chdir(REPO_DIR)\n",
    "    !git pull origin main\n",
    "else:\n",
    "    print(f\"üì• Cloning repository...\")\n",
    "    os.chdir(WORK_DIR)\n",
    "    !git clone {REPO_URL}\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(REPO_DIR))\n",
    "sys.path.insert(0, str(REPO_DIR / \"src\"))\n",
    "sys.path.insert(0, str(REPO_DIR / \"notebooks\"))\n",
    "print(f\"‚úÖ Repository setup complete: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deps-install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# System dependencies\n",
    "if IN_COLAB:\n",
    "    !apt-get update -q\n",
    "    !apt-get install -y ffmpeg libsm6 libxext6\n",
    "\n",
    "# Python packages from requirements.txt or fallback\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# Try to install from requirements.txt if available\n",
    "if Path(\"requirements.txt\").exists():\n",
    "    print(\"Installing from requirements.txt...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "else:\n",
    "    print(\"Installing core dependencies...\")\n",
    "    # Core dependencies\n",
    "    packages = [\n",
    "        \"torch>=2.1\",\n",
    "        \"torchvision\", \n",
    "        \"open_clip_torch>=2.24.0\",\n",
    "        \"faiss-cpu\",\n",
    "        \"pandas>=2.0\",\n",
    "        \"numpy>=1.24\",\n",
    "        \"Pillow>=10.0\",\n",
    "        \"opencv-python\",\n",
    "        \"tqdm\",\n",
    "        \"scikit-learn>=1.4\",\n",
    "        \"rank_bm25>=0.2.2\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "        \"ipywidgets\",\n",
    "        \"decord>=0.6.0\",\n",
    "        \"scipy>=1.11.0\",\n",
    "        \"joblib>=1.3\",\n",
    "        \"pyarrow>=14.0.0\"\n",
    "    ]\n",
    "    \n",
    "    for pkg in packages:\n",
    "        !pip install -q {pkg}\n",
    "\n",
    "# Try GPU FAISS if available\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        !pip uninstall -y -q faiss-cpu faiss-gpu\n",
    "        !pip install -q faiss-gpu\n",
    "        print(\"‚úÖ GPU FAISS installed\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è Using CPU FAISS\")\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## üìö Step 2: Import Libraries & Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports-main"
   },
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, HTML, Image as IPImage, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "# ML imports\n",
    "import torch\n",
    "import faiss\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "import joblib\n",
    "\n",
    "# Import utility functions\n",
    "from colab_utils import (\n",
    "    setup_aic_dataset, \n",
    "    display_search_results,\n",
    "    export_search_results,\n",
    "    evaluate_search_performance,\n",
    "    plot_performance_comparison,\n",
    "    create_training_data_sample,\n",
    "    save_artifacts_summary\n",
    ")\n",
    "\n",
    "# Project imports\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "# Import your actual pipeline modules\n",
    "try:\n",
    "    from src.pipeline.unified_pipeline import UnifiedVideoPipeline\n",
    "    from src.pipeline.query_pipeline import QueryProcessingPipeline\n",
    "    import config\n",
    "    print(\"‚úÖ AIC FTML modules imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Pipeline modules not found: {e}\")\n",
    "    print(\"Will use fallback implementations\")\n",
    "\n",
    "# Create directories\n",
    "directories = [\"data\", \"artifacts\", \"keyframes\", \"output\", \"logs\"]\n",
    "for d in directories:\n",
    "    Path(d).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported and directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üì• Step 3: Dataset Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-config"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "USE_SAMPLE_DATA = True  # Set to False for full AIC dataset\n",
    "SAMPLE_SIZE = 10  # Number of videos for demo\n",
    "TARGET_FRAMES = 50  # Intelligent sampling: best frames per video\n",
    "\n",
    "# Use your actual config values if available\n",
    "try:\n",
    "    MODEL_NAME = config.MODEL_NAME\n",
    "    MODEL_PRETRAINED = config.MODEL_PRETRAINED\n",
    "    ARTIFACT_DIR = config.ARTIFACT_DIR\n",
    "except:\n",
    "    MODEL_NAME = \"ViT-L-16-SigLIP-256\"\n",
    "    MODEL_PRETRAINED = \"webli\"\n",
    "    ARTIFACT_DIR = Path(\"./artifacts\")\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "DATASET_DIR = DATA_DIR / \"videos\"\n",
    "KEYFRAMES_DIR = Path(\"./keyframes\")\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"  Sample mode: {USE_SAMPLE_DATA} ({SAMPLE_SIZE if USE_SAMPLE_DATA else 'full'} videos)\")\n",
    "print(f\"  Intelligent sampling: {TARGET_FRAMES} best frames per video\")\n",
    "print(f\"  Model: {MODEL_NAME} ({MODEL_PRETRAINED})\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Create directories\n",
    "for d in [DATA_DIR, DATASET_DIR, KEYFRAMES_DIR, ARTIFACT_DIR]:\n",
    "    d.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Download and arrange AIC dataset (L21/L22 only)\n",
    "DATASET_ROOT = Path('/content/aic2025') if IN_COLAB else Path('../aic2025')\n",
    "DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print('üì• Downloading dataset archives for L21/L22...')\n",
    "!python scripts/dataset_downloader.py --dataset_root {DATASET_ROOT} --csv AIC_2025_dataset_download_link.csv --videos L21 L22 --skip-existing\n",
    "print('‚úÖ Dataset prepared at', DATASET_ROOT)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-download"
   },
   "outputs": [],
   "source": [
    "# Setup dataset using your AIC CSV or create demo data\n",
    "print(\"üì• Setting up dataset...\")\n",
    "\n",
    "csv_file = Path(\"AIC_2025_dataset_download_link.csv\")\n",
    "dataset_info = setup_aic_dataset(\n",
    "    csv_file=csv_file,\n",
    "    dataset_dir=DATASET_DIR,\n",
    "    use_sample=USE_SAMPLE_DATA,\n",
    "    sample_size=SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset ready - {len(dataset_info)} videos to process\")\n",
    "if len(dataset_info) > 0:\n",
    "    display(dataset_info.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline"
   },
   "source": [
    "## üß† Step 4: Intelligent Video Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline-build"
   },
   "outputs": [],
   "source": "# Initialize your actual pipeline\nprint(\"üöÄ Initializing AIC FTML Pipeline...\")\n\ntry:\n    # Use your actual UnifiedVideoPipeline\n    pipeline = UnifiedVideoPipeline(\n        output_dir=Path(\"./pipeline_output\"),\n        artifact_dir=ARTIFACT_DIR,\n        model_name=MODEL_NAME,\n        pretrained=MODEL_PRETRAINED\n    )\n    \n    print(f\"‚úÖ Pipeline initialized with:\")\n    print(f\"  Model: {MODEL_NAME}\")\n    print(f\"  Pretrained: {MODEL_PRETRAINED}\")\n    print(f\"  Device: {device}\")\n    \n    USE_ACTUAL_PIPELINE = True\n    print(\"üéâ Using real AIC FTML pipeline (not demo fallback)\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Could not initialize actual pipeline: {e}\")\n    print(\"üìù Make sure dependencies are installed:\")\n    print(\"  pip install -r requirements.txt\") \n    print(\"  For GPU: pip install faiss-gpu-cu12 (instead of faiss-cpu)\")\n    print(\"Will create simplified demo version\")\n    USE_ACTUAL_PIPELINE = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build-index"
   },
   "outputs": [],
   "source": "# Build index using your intelligent sampling pipeline\nprint(\"üèóÔ∏è Building Search Index with Intelligent Sampling...\")\n\nif USE_ACTUAL_PIPELINE:\n    # Use your actual pipeline with intelligent sampling\n    print(\"üß† Using AIC FTML intelligent sampling pipeline\")\n    print(\"Features: Visual complexity, scene change detection, motion analysis\")\n    \n    try:\n        # For L21/L22 dataset processing - use real dataset path\n        video_paths = list(DATASET_ROOT.glob(\"videos/*.mp4\"))\n        \n        if not video_paths:\n            print(\"‚ö†Ô∏è No videos found. Checking alternative paths...\")\n            # Try alternative video locations\n            alt_paths = [DATASET_ROOT / \"Videos_L21\", DATASET_ROOT / \"Videos_L22\"]\n            for alt_path in alt_paths:\n                if alt_path.exists():\n                    video_paths.extend(list(alt_path.glob(\"*.mp4\")))\n        \n        if video_paths:\n            print(f\"üìπ Found {len(video_paths)} video files\")\n            \n            # Run your actual build pipeline\n            build_summary = pipeline.build_index(\n                video_paths=video_paths[:5],  # Limit for demo, remove for full dataset\n                target_frames=TARGET_FRAMES,  # Intelligent sampling parameter\n                batch_size=32,\n                use_flat=torch.cuda.is_available(),  # GPU index if available\n                enable_ocr=True,\n                enable_captions=True,\n                enable_segmentation=False\n            )\n            \n            print(\"‚úÖ Index building completed!\")\n            print(f\"Summary: {build_summary}\")\n        else:\n            print(\"‚ö†Ô∏è No videos found - make sure dataset download completed\")\n            USE_ACTUAL_PIPELINE = False\n        \n    except Exception as e:\n        print(f\"‚ùå Pipeline build failed: {e}\")\n        print(\"Will create demo index...\")\n        USE_ACTUAL_PIPELINE = False\n\nif not USE_ACTUAL_PIPELINE:\n    print(\"üîß Creating demo search index...\")\n    \n    # Simplified demo: create fake embeddings and metadata\n    num_frames = len(dataset_info) * TARGET_FRAMES if 'dataset_info' in globals() else 10 * TARGET_FRAMES\n    embedding_dim = 512  # CLIP dimension\n    \n    # Generate random embeddings (replace with actual CLIP encoding)\n    embeddings = np.random.randn(num_frames, embedding_dim).astype(np.float32)\n    embeddings = normalize_rows(embeddings)  # Normalize\n    \n    # Create metadata\n    metadata_rows = []\n    video_ids = [f\"L21_V{i:03d}\" for i in range(1, 11)]  # Demo video IDs\n    \n    for video_id in video_ids:\n        for frame_idx in range(TARGET_FRAMES):\n            metadata_rows.append({\n                'video_id': video_id,\n                'frame_idx': frame_idx,\n                'timestamp': frame_idx * 2.0,  # Every 2 seconds\n                'title': f'HTV News {video_id}',\n                'description': f'News broadcast video {video_id} at {frame_idx * 2.0}s'\n            })\n    \n    metadata_df = pd.DataFrame(metadata_rows)\n    \n    # Build FAISS index\n    d = embedding_dim\n    if torch.cuda.is_available() and num_frames < 50000:\n        index = faiss.IndexFlatIP(d)  # GPU-compatible flat index\n        print(f\"Using flat index for GPU acceleration\")\n    else:\n        index = faiss.IndexHNSWFlat(d, 32)\n        index.hnsw.efConstruction = 200\n        print(f\"Using HNSW index for CPU\")\n    \n    index.add(embeddings)\n    \n    # Save artifacts\n    faiss.write_index(index, str(ARTIFACT_DIR / \"vector_index.faiss\"))\n    metadata_df.to_parquet(ARTIFACT_DIR / \"index_metadata.parquet\", index=False)\n    \n    print(f\"‚úÖ Demo index built:\")\n    print(f\"  Vectors: {index.ntotal}\")\n    print(f\"  Dimension: {index.d}\")\n    print(f\"  Metadata entries: {len(metadata_df)}\")\n    print(f\"  Videos: {metadata_df['video_id'].nunique()}\")\n    print(f\"  Avg frames per video: {len(metadata_df) / metadata_df['video_id'].nunique():.1f}\")\n\nprint(\"\\nüéâ Intelligent sampling and indexing complete!\")\nif USE_ACTUAL_PIPELINE:\n    print(\"Your system now contains intelligently sampled keyframes with 70-90% storage reduction.\")\nelse:\n    print(\"Demo system created - replace with real dataset for full functionality.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "search-interface"
   },
   "source": [
    "## üîç Step 5: Search Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "search-setup"
   },
   "outputs": [],
   "source": "# Initialize search system\nprint(\"üîç Initializing Search System...\")\n\nif USE_ACTUAL_PIPELINE:\n    # Use your actual query pipeline\n    try:\n        query_pipeline = QueryProcessingPipeline(\n            artifact_dir=ARTIFACT_DIR,\n            model_name=MODEL_NAME,\n            pretrained=MODEL_PRETRAINED,\n            enable_reranking=True\n        )\n        print(\"‚úÖ Using actual QueryProcessingPipeline\")\n        print(\"Features: Hybrid search, ML reranking, temporal deduplication\")\n        \n        # Wrapper function for consistent interface\n        def search_function(query, mode='hybrid', k=20):\n            return query_pipeline.search(\n                query=query,\n                search_mode=mode,\n                k=k\n            )\n            \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not initialize query pipeline: {e}\")\n        USE_ACTUAL_PIPELINE = False\n\nif not USE_ACTUAL_PIPELINE:\n    print(\"üîß Creating demo search function...\")\n    \n    # Load demo artifacts\n    try:\n        index = faiss.read_index(str(ARTIFACT_DIR / \"vector_index.faiss\"))\n        metadata_df = pd.read_parquet(ARTIFACT_DIR / \"index_metadata.parquet\")\n    except:\n        print(\"‚ö†Ô∏è No artifacts found, creating minimal demo data\")\n        # Create minimal demo data\n        metadata_df = pd.DataFrame([\n            {'video_id': 'L21_V001', 'frame_idx': 0, 'timestamp': 0.0},\n            {'video_id': 'L21_V001', 'frame_idx': 30, 'timestamp': 1.0},\n            {'video_id': 'L21_V002', 'frame_idx': 0, 'timestamp': 0.0},\n        ])\n    \n    # Simple demo search function\n    def search_function(query, mode='hybrid', k=20):\n        # Simple demo: return random results with decreasing scores\n        num_results = min(k, len(metadata_df))\n        sample_results = metadata_df.sample(num_results).reset_index(drop=True)\n        \n        # Create mock SearchResult objects\n        results = []\n        for i, (_, row) in enumerate(sample_results.iterrows()):\n            # Mock result object\n            result = type('SearchResult', (), {\n                'video_id': row['video_id'],\n                'frame_idx': row['frame_idx'],\n                'score': 0.9 - (i * 0.03),  # Decreasing scores\n                'metadata': {'search_type': mode}\n            })()\n            results.append(result)\n        \n        return results\n\nprint(\"‚úÖ Search system ready\")\n\n# Test search\ntest_results = search_function(\"news anchor\", k=5)\nprint(f\"üß™ Test search returned {len(test_results)} results\")\nif USE_ACTUAL_PIPELINE:\n    print(\"üéØ Real pipeline active - results from actual CLIP embeddings and hybrid search\")\nelse:\n    print(\"üìã Demo mode - replace with real dataset for actual search results\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "search-widget"
   },
   "outputs": [],
   "source": [
    "# Interactive search interface\n",
    "print(\"üéõÔ∏è Interactive Search Interface\")\n",
    "\n",
    "# Search parameters\n",
    "query_widget = widgets.Text(\n",
    "    value='news anchor speaking',\n",
    "    placeholder='Enter your search query...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='400px'),\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "search_mode_widget = widgets.Dropdown(\n",
    "    options=[('Hybrid (Best)', 'hybrid'), ('Vector Only', 'vector'), ('Text Only', 'text')],\n",
    "    value='hybrid',\n",
    "    description='Mode:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "k_widget = widgets.IntSlider(\n",
    "    value=20,\n",
    "    min=5,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='Results:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "max_display_widget = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=30,\n",
    "    step=5,\n",
    "    description='Display:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "# Interactive search function\n",
    "def interactive_search(query, search_mode, k, max_display):\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a search query\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    print(f\"Mode: {search_mode}, Results: {k}, Display: {max_display}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = search_function(query, search_mode, k)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Search completed in {search_time*1000:.1f}ms\")\n",
    "    \n",
    "    if results:\n",
    "        display_search_results(results, query, max_display, KEYFRAMES_DIR)\n",
    "    else:\n",
    "        print(\"‚ùå No results found\")\n",
    "\n",
    "# Create interactive widget\n",
    "search_widget = interactive(\n",
    "    interactive_search,\n",
    "    query=query_widget,\n",
    "    search_mode=search_mode_widget,\n",
    "    k=k_widget,\n",
    "    max_display=max_display_widget\n",
    ")\n",
    "\n",
    "display(search_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## üéØ Step 6: Training & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-data"
   },
   "outputs": [],
   "source": [
    "# Generate training data for reranking\n",
    "print(\"üìä Generating Training Data...\")\n",
    "\n",
    "ENABLE_TRAINING = True  # Set to False to skip\n",
    "\n",
    "if ENABLE_TRAINING:\n",
    "    # Load metadata for training data generation\n",
    "    try:\n",
    "        metadata_df = pd.read_parquet(ARTIFACT_DIR / \"index_metadata.parquet\")\n",
    "        \n",
    "        # Generate training examples\n",
    "        training_data = create_training_data_sample(metadata_df, num_examples=30)\n",
    "        \n",
    "        # Save training data\n",
    "        training_file = DATA_DIR / \"train.jsonl\"\n",
    "        with open(training_file, 'w') as f:\n",
    "            for item in training_data:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(training_data)} training examples\")\n",
    "        print(f\"Sample query: '{training_data[0]['query']}' -> {len(training_data[0]['positives'])} positives\")\n",
    "        \n",
    "        # Simple reranker training (basic logistic regression)\n",
    "        print(\"\\nüéØ Training Simple Reranker...\")\n",
    "        \n",
    "        # Create simple feature vectors and train basic model\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        \n",
    "        for item in training_data:\n",
    "            query = item['query']\n",
    "            # Simple features: query length, word count, etc.\n",
    "            for pos in item['positives']:\n",
    "                features = [\n",
    "                    len(query),\n",
    "                    len(query.split()),\n",
    "                    1 if 'news' in query.lower() else 0,\n",
    "                    1 if 'anchor' in query.lower() else 0,\n",
    "                    pos['frame_idx'] / 100.0  # Normalized frame position\n",
    "                ]\n",
    "                X_train.append(features)\n",
    "                y_train.append(1)  # Positive example\n",
    "                \n",
    "                # Add negative example\n",
    "                neg_features = features.copy()\n",
    "                neg_features[-1] = np.random.random()  # Random frame position\n",
    "                X_train.append(neg_features)\n",
    "                y_train.append(0)  # Negative example\n",
    "        \n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        \n",
    "        if len(np.unique(y_train)) > 1:\n",
    "            reranker = LogisticRegression(random_state=42)\n",
    "            reranker.fit(X_train, y_train)\n",
    "            \n",
    "            # Save model\n",
    "            joblib.dump(reranker, ARTIFACT_DIR / \"simple_reranker.joblib\")\n",
    "            print(f\"‚úÖ Simple reranker trained and saved\")\n",
    "            print(f\"Training accuracy: {reranker.score(X_train, y_train):.3f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Not enough diverse training data\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è Training skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## üìä Step 7: Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation-run"
   },
   "outputs": [],
   "source": [
    "# Performance evaluation\n",
    "print(\"üìà Performance Evaluation\")\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_search_performance(search_function)\n",
    "\n",
    "if len(eval_results) > 0:\n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    display(eval_results.round(3))\n",
    "    \n",
    "    # Plot performance comparison\n",
    "    plot_performance_comparison(eval_results)\n",
    "    \n",
    "    # Performance summary\n",
    "    summary = eval_results.groupby('mode')[['search_time_ms', 'avg_score', 'diversity']].mean()\n",
    "    print(\"\\n‚ö° Performance Summary:\")\n",
    "    best_mode = summary['avg_score'].idxmax()\n",
    "    fastest_mode = summary['search_time_ms'].idxmin()\n",
    "    print(f\"  Best quality: {best_mode} (avg score: {summary.loc[best_mode, 'avg_score']:.3f})\")\n",
    "    print(f\"  Fastest: {fastest_mode} ({summary.loc[fastest_mode, 'search_time_ms']:.1f}ms avg)\")\n",
    "    print(f\"  Recommended: hybrid (balanced performance)\")\nelse:\n",
    "    print(\"‚ö†Ô∏è No evaluation results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export"
   },
   "source": [
    "## üíæ Step 8: Export & Results Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export-widget"
   },
   "outputs": [],
   "source": [
    "# Export functionality\n",
    "print(\"üíæ Export Search Results\")\n",
    "\n",
    "export_query_widget = widgets.Text(\n",
    "    value='news anchor speaking',\n",
    "    placeholder='Query to search and export...',\n",
    "    description='Query:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "export_format_widget = widgets.Dropdown(\n",
    "    options=['csv', 'json', 'parquet'],\n",
    "    value='csv',\n",
    "    description='Format:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "export_k_widget = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=10,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Results:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "def do_export(query, format_type, k):\n",
    "    if not query.strip():\n",
    "        print(\"Please enter a query\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Searching for '{query}' (k={k})...\")\n",
    "    results = search_function(query, k=k)\n",
    "    print(f\"Found {len(results)} results\")\n",
    "    \n",
    "    if results:\n",
    "        filename = export_search_results(results, query, format_type)\n",
    "        print(f\"üìÑ Results exported to: {filename}\")\n",
    "    else:\n",
    "        print(\"‚ùå No results to export\")\n",
    "\n",
    "export_widget = interactive(\n",
    "    do_export,\n",
    "    query=export_query_widget,\n",
    "    format_type=export_format_widget,\n",
    "    k=export_k_widget\n",
    ")\n",
    "\n",
    "display(export_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìã Step 9: Final Summary & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create training data from metadata and train reranker\n",
    "print('üß™ Generating training data and training reranker...')\n",
    "!python scripts/create_training_data.py --dataset_root {DATASET_ROOT} --output data/train.jsonl --num_examples 80\n",
    "!python src/training/train_reranker.py --index_dir {ARTIFACT_DIR} --train_jsonl data/train.jsonl\n",
    "print('‚úÖ Reranker trained and saved to artifacts')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "print(\"üéâ AIC FTML Pipeline Complete!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# System summary\n",
    "try:\n",
    "    metadata_df = pd.read_parquet(ARTIFACT_DIR / \"index_metadata.parquet\")\n",
    "    \n",
    "    print(f\"üìä System Summary:\")\n",
    "    print(f\"  Total videos processed: {metadata_df['video_id'].nunique()}\")\n",
    "    print(f\"  Total frames indexed: {len(metadata_df)}\")\n",
    "    print(f\"  Avg frames per video: {len(metadata_df) / metadata_df['video_id'].nunique():.1f}\")\n",
    "    print(f\"  Search modes: vector, text, hybrid\")\n",
    "    print(f\"  Model: {MODEL_NAME} ({MODEL_PRETRAINED})\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    if USE_ACTUAL_PIPELINE:\n",
    "        print(f\"\\nüß† Intelligent Sampling Features:\")\n",
    "        print(f\"  ‚úÖ Visual complexity scoring\")\n",
    "        print(f\"  ‚úÖ Scene change detection\")\n",
    "        print(f\"  ‚úÖ Motion analysis\")\n",
    "        print(f\"  ‚úÖ Semantic importance weighting\")\n",
    "        print(f\"  ‚úÖ Smart deduplication\")\n",
    "        print(f\"  üìà Storage reduction: 70-90%\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load summary data: {e}\")\n",
    "\n",
    "# Artifacts summary\n",
    "print(f\"\\nüìÅ Generated Artifacts:\")\n",
    "artifacts_summary = save_artifacts_summary(ARTIFACT_DIR)\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Production!\")\n",
    "print(f\"  ‚úÖ Search interface ready\")\n",
    "print(f\"  ‚úÖ Export functionality available\")\n",
    "print(f\"  ‚úÖ All artifacts saved for reuse\")\n",
    "print(f\"  ‚úÖ Scalable to full AIC dataset\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Set USE_SAMPLE_DATA=False for full dataset\")\n",
    "print(f\"  2. Upload your AIC_2025_dataset_download_link.csv\")\n",
    "print(f\"  3. Adjust TARGET_FRAMES based on your needs\")\n",
    "print(f\"  4. Export results for competition submission\")\n",
    "print(f\"  5. Fine-tune models with your specific data\")\n",
    "\n",
    "print(f\"\\nüéâ Happy searching with AIC FTML!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "accelerator": "GPU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}