{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIC 2024/2025 Official – Inference + Evaluation\n",
        "This Colab runs host inference, writes per-query CSVs named `{query_id}.csv`, and evaluates with the official scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System check\n",
        "!nvidia-smi || true\n",
        "!python --version\n",
        "import os, sys, pathlib; print('CWD:', os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: clone repo and install deps (auto FAISS CPU/GPU)\n",
        "REPO_URL = 'https://github.com/nqvu-daniel/AIC_FTML_dev.git'\n",
        "REPO_NAME = 'AIC_FTML_dev'\n",
        "import subprocess, pathlib, sys, os\n",
        "if not pathlib.Path(REPO_NAME).exists():\n",
        "    subprocess.run(['git', 'clone', REPO_URL], check=True)\n",
        "%cd {REPO_NAME}\n",
        "!python -m pip install -q -r requirements.txt\n",
        "# Install FAISS CPU/GPU based on CUDA availability\n",
        "import sys as _sys, subprocess as _subp\n",
        "try:\n",
        "    import torch as _torch\n",
        "    _subp.run([_sys.executable, '-m', 'pip', 'uninstall', '-y', 'faiss-cpu', 'faiss-gpu'], check=False, stdout=_subp.PIPE, stderr=_subp.PIPE)\n",
        "    _pkg = 'faiss-gpu==1.7.4' if _torch.cuda.is_available() else 'faiss-cpu==1.7.4'\n",
        "    _subp.run([_sys.executable, '-m', 'pip', 'install', '-q', _pkg], check=True)\n",
        "    print('Installed', _pkg)\n",
        "except Exception as _e:\n",
        "    print('FAISS install fallback error:', _e)\n",
        "print('Setup done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference (KIS/VQA) – Official File Naming\n",
        "- Provide `ARTIFACTS_BUNDLE_URL` and/or `RERANKER_MODEL_URL` if needed.\n",
        "- Output path auto-uses `submissions/{query_id}.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure your query\n",
        "QUERY = 'a person opening a laptop'\n",
        "QUERY_ID = 'q1'  # official id\n",
        "TASK = 'kis'     # 'kis' or 'vqa'\n",
        "ANSWER = ''      # required if TASK='vqa'\n",
        "ARTIFACTS_BUNDLE_URL = ''  # optional\n",
        "RERANKER_MODEL_URL = ''    # optional\n",
        "\n",
        "# Run inference\n",
        "cmd = ['python', 'src/retrieval/use.py', '--query', QUERY, '--query_id', QUERY_ID, '--task', TASK]\n",
        "if TASK == 'vqa':\n",
        "    assert ANSWER, 'For TASK=vqa, set ANSWER'\n",
        "    cmd += ['--answer', ANSWER]\n",
        "if ARTIFACTS_BUNDLE_URL:\n",
        "    cmd += ['--bundle_url', ARTIFACTS_BUNDLE_URL]\n",
        "if RERANKER_MODEL_URL:\n",
        "    cmd += ['--model_url', RERANKER_MODEL_URL]\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        cmd += ['--faiss_gpu']\n",
        "except Exception:\n",
        "    pass\n",
        "print('Running:', ' '.join(cmd))\n",
        "import subprocess\n",
        "res = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(res.stdout)\n",
        "if res.returncode:\n",
        "    print(res.stderr)\n",
        "    raise SystemExit(res.returncode)\n",
        "!ls -l submissions | head -n 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate\n",
        "Place/point to your ground truth JSON and select task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to ground truth (e.g., from Drive)\n",
        "GT_PATH = 'ground_truth.json'  # update to your path\n",
        "TASK_EVAL = TASK               # 'kis' or 'vqa' or 'trake'\n",
        "NORMALIZE_ANS = False          # set True if needed for VQA\n",
        "\n",
        "cmd = ['python', 'eval/evaluate.py', '--gt', GT_PATH, '--pred_dir', 'submissions', '--task', TASK_EVAL]\n",
        "if NORMALIZE_ANS:\n",
        "    cmd.append('--normalize_answer')\n",
        "print('Evaluating:', ' '.join(cmd))\n",
        "import subprocess\n",
        "res = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(res.stdout)\n",
        "if res.returncode:\n",
        "    print(res.stderr)\n",
        "    raise SystemExit(res.returncode)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
