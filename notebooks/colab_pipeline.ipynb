{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca765bd",
   "metadata": {
    "id": "0ca765bd"
   },
   "source": [
    "## System Check & Setup\n",
    "\n",
    "Run this cell first to check your Colab environment and system capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1a9d7",
   "metadata": {},
   "source": "## NEW: Segment-first + ASR + CE Re-rank + TransNetV2\nThis notebook now supports a stronger retrieval flow:\n- **NEW**: Segment videos with TransNetV2 (deep learning shot detection) → `segments.parquet`\n- Build index over segment representative frames using GPU-accelerated FAISS\n- Merge ASR transcripts into the corpus (optional)\n- Hybrid retrieval (BM25 + FAISS) with optional cross-encoder re-ranking\n- Full support for KIS, VQA, and TRAKE (host performs scoring)\n\n### Quick CLI (local or Colab VM)\n````bash\n# 1) Segment videos with TransNetV2 (collection IDs like L21 or explicit IDs)\npython scripts/segment_videos.py --dataset_root /content/aic2025 \\\n  --videos L21 L22 --artifact_dir ./artifacts --use_transnetv2\n\n# 2) Build index using segment reps with GPU acceleration\npython scripts/index.py --dataset_root /content/aic2025 \\\n  --videos L21 L22 --segments ./artifacts/segments.parquet\n\n# 3) Build text corpus (merge ASR if available)\npython scripts/build_text.py --dataset_root /content/aic2025 \\\n  --videos L21 L22 --artifact_dir ./artifacts \\\n  --segments ./artifacts/segments.parquet \\\n  --transcripts /content/transcripts.jsonl   # optional\n\n# 4) Search examples\npython src/retrieval/use.py --query \"your search\" --query_id q1 --rerank ce\npython src/retrieval/use.py --task vqa --query \"câu hỏi\" --answer \"màu xanh\" --query_id q2 --rerank ce\npython src/retrieval/use.py --task trake --query \"high jump\" \\\n  --events_json /content/events.json --query_id q3 --rerank ce\n````\n\n**Key Updates:**\n- **TransNetV2**: Deep learning-based shot boundary detection (replaces OpenCV fallback)\n- **GPU FAISS**: Uses `faiss-gpu-cu12` for faster indexing and search\n- **Better Dependencies**: Latest `open-clip-torch` with improved multilingual support\n\nNotes: We export CSVs only; the host computes official scores."
  },
  {
   "cell_type": "markdown",
   "id": "1e2adv54bvy",
   "metadata": {
    "id": "1e2adv54bvy"
   },
   "source": "# AIC 2024/2025 Retrieval – Automated Pipeline ⚡\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nqvu-daniel/AIC_FTML_dev/blob/main/notebooks/colab_pipeline.ipynb)\n\n## 🚀 **NEW: Segment-First Pipeline with TransNetV2**\n**Latest update**: Modern **segment-first architecture** with **TransNetV2** deep learning shot detection!\n\n**Quick Start:**\n1. **Enable GPU**: Runtime → Change runtime type → Hardware accelerator: T4/L4/A100 (recommended for Colab)\n2. **Run Setup**: Execute the \"Setup\" cell below to automatically clone repo and install dependencies\n3. **Configure Environment**: Set `IS_COLAB = True/False` in the configuration cell\n4. **Choose Your Path**:\n   - Host Inference (recommended): Use pre-built artifacts to run queries instantly\n   - Development Pipeline: Build your own artifacts with **segment-first approach**\n\n**File Downloads**: \n- **Colab**: Results saved to `/content/AIC_FTML_dev/submissions/` - download from Colab's file browser\n- **Local**: Results saved to `./submissions/` in your current directory\n\n---\n\n## Two Usage Modes\n\n### 1. Host Inference (Recommended - Fast)\n- No dataset required\n- Uses pre-built artifacts and models\n- Ready in ~2 minutes\n- Perfect for running queries and getting CSV results\n\n### 2. Development Pipeline (Advanced - Modern Segment-First!)\n- Downloads full dataset (~GBs)\n- **NEW**: Segment-first architecture with TransNetV2 shot detection\n- **NEW**: GPU-accelerated FAISS indexing\n- **NEW**: Proper text corpus with ASR integration\n- Builds search index + custom reranker models\n\n---\n\n## 🎯 Modern Pipeline Architecture\n\n| Component | Technology | Purpose |\n|-----------|-------------|---------|\n| **Video Segmentation** | TransNetV2 (deep learning) | Accurate shot boundary detection |\n| **Visual Index** | FAISS GPU | Fast similarity search |\n| **Text Corpus** | BM25 + ASR merge | Hybrid text search |\n| **Reranking** | Cross-encoder | Result quality boost |\n| **Dependencies** | Latest open-clip-torch | Better multilingual support |\n\n**Key Improvements:**\n- Replaces old \"smart pipeline\" with proper segment-first approach\n- Uses TransNetV2 instead of basic OpenCV detection\n- GPU-accelerated processing throughout\n- Structured 3-step build process\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KOTbWBx_jr0I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOTbWBx_jr0I",
    "outputId": "c250356e-b576-49be-d8a2-1fbe9c83874c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi || true\n",
    "!python --version\n",
    "import sys, os, pathlib\n",
    "print('CWD:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d80459",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (CUDA 12.1)\n!pip -q install --upgrade pip\n\n# Clean potentially conflicting preinstalled packages (safe to ignore errors)\n!pip -q uninstall -y opencv-python opencv-contrib-python opencv-python-headless numpy scipy pandas scikit-learn faiss faiss-cpu faiss-gpu faiss-gpu-cu12 faiss-cpu-cu12 open-clip-torch thinc spacy pillow decord pyarrow  || true\n!pip -q cache purge || true\n\n# PyTorch cu121\n!pip -q install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n\n# Align to NumPy 2 to avoid ABI conflicts with OpenCV 4.12/thinc\n!pip -q install \"numpy>=2.0,<2.3\" \"scipy>=1.13.1\" \"pandas>=2.2.2\" \"scikit-learn>=1.5.1\"\n\n# Core libraries with pinned versions (headless OpenCV 4.12 needs NumPy 2.x)\n!pip -q install open-clip-torch>=2.24.0 rank_bm25 joblib pyarrow tqdm pyyaml pillow \"opencv-python-headless==4.12.0.88\" decord transnetv2-pytorch>=1.0.0\n\n# Install GPU-optimized FAISS for CUDA 12\n!pip -q install faiss-gpu-cu12\n\n# Verify\nimport torch, sys\nprint('Torch', torch.__version__, 'CUDA', torch.version.cuda, 'CUDA available', torch.cuda.is_available())\ntry:\n    import faiss\n    print('FAISS', getattr(faiss, '__version__', 'n/a'))\n    # Check if GPU FAISS is available\n    if hasattr(faiss, 'StandardGpuResources'):\n        print('FAISS GPU support: Available')\n    else:\n        print('FAISS GPU support: Not available')\nexcept Exception as e:\n    print('FAISS import error:', e)\ntry:\n    import open_clip\n    print('open_clip', getattr(open_clip, '__version__', 'n/a'))\nexcept Exception as e:\n    print('open_clip import error:', e)\ntry:\n    import transnetv2_pytorch\n    print('TransNetV2', getattr(transnetv2_pytorch, '__version__', 'available'))\nexcept Exception as e:\n    print('TransNetV2 import error:', e)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5e6ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebf5e6ae",
    "outputId": "7d9df5aa-3bce-4ddd-8ae0-f1ee66f45985"
   },
   "outputs": [],
   "source": "# Setup: Clone repo and install dependencies automatically (GPU-ready)\nimport os\nimport pathlib\nimport subprocess\nimport sys\n\nREPO_URL = 'https://github.com/nqvu-daniel/AIC_FTML_dev.git'\nREPO_NAME = 'AIC_FTML_dev'\n\ndef setup_repository():\n    \"\"\"Automatically clone repository and setup environment\"\"\"\n    try:\n        # Check if repo already exists\n        if pathlib.Path(REPO_NAME).exists():\n            print(f\"Repository '{REPO_NAME}' already exists\")\n            os.chdir(REPO_NAME)\n        else:\n            print(f\"Cloning repository from {REPO_URL}\")\n            subprocess.run(['git', 'clone', REPO_URL], check=True)\n            os.chdir(REPO_NAME)\n            print(\"Repository cloned successfully\")\n\n        # Install dependencies\n        print(\"Installing dependencies...\")\n        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'], check=True)\n\n        # Install GPU-optimized FAISS and TransNetV2\n        try:\n            import torch\n            if torch.cuda.is_available():\n                print(\"GPU detected, installing faiss-gpu-cu12...\")\n                subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-gpu-cu12'], check=True)\n                print('Installed faiss-gpu-cu12 (CUDA 12 compatible)')\n            else:\n                print(\"No GPU detected, installing faiss-cpu...\")\n                subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-cpu'], check=True)\n                print('Installed faiss-cpu')\n            \n            # Install TransNetV2 for advanced video segmentation\n            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'transnetv2-pytorch>=1.0.0'], check=True)\n            print('Installed TransNetV2-PyTorch for video segmentation')\n            \n        except Exception as e:\n            print(f'FAISS/TransNetV2 install error: {e}')\n            # Fallback to CPU versions\n            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-cpu'], check=True)\n            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'transnetv2-pytorch'], check=True)\n            print('Fallback: Installed faiss-cpu and transnetv2-pytorch')\n\n        # Add to Python path\n        if '.' not in sys.path:\n            sys.path.append('.')\n\n        print(\"Setup complete! Ready to run AIC FTML pipeline\")\n        print(f\"Current directory: {os.getcwd()}\")\n\n        return True\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error during setup: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return False\n\n# Run setup\nif setup_repository():\n    print(\"\\nYou can now proceed with the pipeline!\")\nelse:\n    print(\"\\nSetup failed. Please check the errors above.\")"
  },
  {
   "cell_type": "markdown",
   "id": "CBpMdVd0jr0I",
   "metadata": {
    "id": "CBpMdVd0jr0I"
   },
   "source": [
    "## Host Inference – One-shot\n",
    "Provide `ARTIFACTS_BUNDLE_URL` and/or `RERANKER_MODEL_URL` if not already present in `./artifacts`.\n",
    "This writes a Top-100 CSV into `submissions/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74133bd3",
   "metadata": {
    "id": "74133bd3"
   },
   "outputs": [],
   "source": [
    "# Host Inference - Automated Setup and Query Execution\n",
    "import os\n",
    "import subprocess\n",
    "import pathlib\n",
    "\n",
    "# Configuration - Update these URLs with your hosted models\n",
    "QUERY = 'a person opening a laptop'  # Change this to your search query\n",
    "QUERY_ID = 'q1'  # Official query id for filename submissions/{query_id}.csv\n",
    "TASK = 'kis'     # 'kis' or 'vqa'\n",
    "ANSWER = ''      # Required if TASK='vqa'\n",
    "ARTIFACTS_BUNDLE_URL = ''  # e.g., 'https://your-host.com/artifacts_bundle.tar.gz'\n",
    "RERANKER_MODEL_URL = ''    # e.g., 'https://your-host.com/reranker.joblib'\n",
    "\n",
    "def run_inference_query(query, bundle_url='', model_url='', query_id='', task='kis', answer=''):\n",
    "    \"\"\"Run inference with automatic artifact download if needed\"\"\"\n",
    "    try:\n",
    "        # Ensure we're in the right directory\n",
    "        if not pathlib.Path('src/retrieval/use.py').exists():\n",
    "            print('Missing use.py script. Make sure setup completed successfully.')\n",
    "            return False\n",
    "\n",
    "        # Build command\n",
    "        cmd = ['python', 'src/retrieval/use.py', '--query', query, '--task', task]\n",
    "        if query_id:\n",
    "            cmd.extend(['--query_id', query_id])\n",
    "        if task == 'vqa':\n",
    "            if not answer:\n",
    "                print('For TASK=vqa you must set ANSWER.')\n",
    "                return False\n",
    "            cmd.extend(['--answer', answer])\n",
    "\n",
    "        if bundle_url:\n",
    "            cmd.extend(['--bundle_url', bundle_url])\n",
    "            print(f'Will download artifacts bundle from: {bundle_url}')\n",
    "\n",
    "        if model_url:\n",
    "            cmd.extend(['--model_url', model_url])\n",
    "            print(f'Will download reranker model from: {model_url}')\n",
    "\n",
    "        # Create submissions directory if it doesn't exist\n",
    "        os.makedirs('submissions', exist_ok=True)\n",
    "\n",
    "        print(f\"Running query: '{query}' (task={task}, qid={query_id})\")\n",
    "        print('Command:', ' '.join(cmd))\n",
    "\n",
    "        # Execute the command\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print('Query execution successful!')\n",
    "            if result.stdout:\n",
    "                print('Output:\\n' + result.stdout)\n",
    "\n",
    "            # List generated files\n",
    "            submissions_dir = pathlib.Path('submissions')\n",
    "            if submissions_dir.exists():\n",
    "                csv_files = list(submissions_dir.glob('*.csv'))\n",
    "                if csv_files:\n",
    "                    print('\\nGenerated ' + str(len(csv_files)) + ' result file(s):')\n",
    "                    for csv_file in csv_files:\n",
    "                        print(f'  - {csv_file}')\n",
    "                        # Show first few lines of the CSV\n",
    "                        try:\n",
    "                            with open(csv_file, 'r') as f:\n",
    "                                lines = f.readlines()[:5]\n",
    "                                print('    Preview (first 5 lines):')\n",
    "                                for i, line in enumerate(lines, 1):\n",
    "                                    print(f'    {i}: {line.strip()}')\n",
    "                        except Exception as e:\n",
    "                            print(f'    (Could not preview: {e})')\n",
    "            return True\n",
    "        else:\n",
    "            print('Query execution failed!')\n",
    "            print('Error output:\\n' + result.stderr)\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error running inference: {e}')\n",
    "        return False\n",
    "\n",
    "# Run the inference\n",
    "print('Starting AIC FTML Host Inference...')\n",
    "success = run_inference_query(QUERY, ARTIFACTS_BUNDLE_URL, RERANKER_MODEL_URL, QUERY_ID, TASK, ANSWER)\n",
    "\n",
    "if success:\n",
    "    print('\\nInference completed! Check the submissions/ folder for results.')\n",
    "else:\n",
    "    print('\\nInference failed. Check the error messages above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefaa0b",
   "metadata": {
    "id": "5eefaa0b"
   },
   "source": [
    "## Dev Pipeline – Build Artifacts (Optional)\n",
    "Downloads dataset archives using `AIC_2025_dataset_download_link.csv`, builds index/corpus, optionally trains reranker, and assembles `my_pipeline/`.\n",
    "\n",
    "### Configuration\n",
    "Set your preferences here - run this cell first to configure the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qcp73p5wree",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcp73p5wree",
    "outputId": "706b47f9-b2c4-4465-d9a7-070f9181bb2b"
   },
   "outputs": [],
   "source": [
    "# Configuration - Run this cell first\n",
    "import os\n",
    "import subprocess\n",
    "import pathlib\n",
    "import time\n",
    "import csv\n",
    "import tempfile\n",
    "\n",
    "# Configuration - Set your environment\n",
    "IS_COLAB = True  # Set to False for local environment\n",
    "\n",
    "# Configuration - Dataset root based on environment\n",
    "# Both Colab and local keep dataset alongside AIC_FTML_dev directory\n",
    "DATASET_ROOT = '/content/aic2025' if IS_COLAB else '../aic2025'\n",
    "TEST_MODE = True  # Uncomment to enable test mode (only downloads L21-L24)\n",
    "VIDEOS = ['L21', 'L22', 'L23', 'L24', 'L25', 'L26', 'L27', 'L28', 'L29', 'L30']  # adjust if needed\n",
    "CSV_FILE = 'AIC_2025_dataset_download_link.csv'  # Update path if different\n",
    "\n",
    "# Apply test mode if enabled\n",
    "try:\n",
    "    if TEST_MODE:\n",
    "        VIDEOS = ['L21', 'L22']\n",
    "        print(\"TEST MODE ENABLED: Only processing L21-L24\")\n",
    "except NameError:\n",
    "    print(\"Using full video list:\", VIDEOS)\n",
    "\n",
    "def filter_csv_for_videos(csv_path, video_list, output_path):\n",
    "    \"\"\"Filter the CSV file to only include entries for specified videos + essential metadata\"\"\"\n",
    "    if not pathlib.Path(csv_path).exists():\n",
    "        return False\n",
    "\n",
    "    filtered_rows = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader, None)\n",
    "        if header:\n",
    "            filtered_rows.append(header)\n",
    "\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            # Check if any of our target videos appear in the filename\n",
    "            filename = row[-2].strip() if len(row) >= 2 else \"\"\n",
    "            filename_upper = filename.upper()\n",
    "\n",
    "            # Always include essential metadata files (needed for all videos)\n",
    "            essential_files = [\n",
    "                'MAP-KEYFRAMES-AIC25-B1.ZIP',\n",
    "                'MEDIA-INFO-AIC25-B1.ZIP',\n",
    "                'OBJECTS-AIC25-B1.ZIP',\n",
    "                'CLIP-FEATURES-32-AIC25-B1.ZIP'\n",
    "            ]\n",
    "\n",
    "            is_essential = any(essential in filename_upper for essential in essential_files)\n",
    "            is_target_video = any(vid in filename_upper for vid in video_list)\n",
    "\n",
    "            if is_essential or is_target_video:\n",
    "                filtered_rows.append(row)\n",
    "\n",
    "    # Write filtered CSV\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(filtered_rows)\n",
    "\n",
    "    print(f\"Filtered CSV: {len(filtered_rows)-1} entries for videos {video_list} + essential metadata\")\n",
    "    return True\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Environment: {'Google Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Dataset root: {DATASET_ROOT}\")\n",
    "print(f\"Videos to process: {VIDEOS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C16Zg3aWml_D",
   "metadata": {
    "id": "C16Zg3aWml_D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4wggtvfr38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4wggtvfr38",
    "outputId": "ffda53de-aedf-424b-9423-a5967535f94b"
   },
   "outputs": [],
   "source": [
    "# Step 1: Download Dataset (Skip this cell if data already downloaded)\n",
    "from tqdm import tqdm\n",
    "print(\"Step 1: Download dataset\")\n",
    "start_time = time.time()\n",
    "\n",
    "if pathlib.Path(CSV_FILE).exists():\n",
    "    # Create filtered CSV for our target videos\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp_csv:\n",
    "        filtered_csv_path = tmp_csv.name\n",
    "\n",
    "    if filter_csv_for_videos(CSV_FILE, VIDEOS, filtered_csv_path):\n",
    "        print(\"Starting dataset download with progress tracking...\")\n",
    "        cmd = [\n",
    "            'python', 'scripts/dataset_downloader.py',\n",
    "            '--dataset_root', DATASET_ROOT,\n",
    "            '--csv', filtered_csv_path,\n",
    "            '--skip-existing'\n",
    "        ]\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "        # Run with real-time output to show download progress\n",
    "        import sys\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        return_code = process.poll()\n",
    "\n",
    "        # Clean up temp file\n",
    "        try:\n",
    "            os.unlink(filtered_csv_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if return_code != 0:\n",
    "            print(\"Dataset download failed!\")\n",
    "            raise Exception(\"Download failed\")\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Dataset download completed in {elapsed:.1f} seconds\")\n",
    "\n",
    "        # Debug: Check what was actually extracted\n",
    "        print(\"\\nChecking extracted structure:\")\n",
    "        dataset_path = pathlib.Path(DATASET_ROOT)\n",
    "        if dataset_path.exists():\n",
    "            for subdir in ['videos', 'keyframes', 'map_keyframes', 'media_info', 'objects', 'features']:\n",
    "                subdir_path = dataset_path / subdir\n",
    "                if subdir_path.exists():\n",
    "                    files = list(subdir_path.rglob('*'))\n",
    "                    print(f\"  {subdir}/: {len(files)} items\")\n",
    "                    # Show first few items\n",
    "                    for item in files[:5]:\n",
    "                        rel_path = item.relative_to(subdir_path)\n",
    "                        item_type = \"DIR\" if item.is_dir() else \"FILE\"\n",
    "                        print(f\"    {item_type}: {rel_path}\")\n",
    "                    if len(files) > 5:\n",
    "                        print(f\"    ... and {len(files) - 5} more\")\n",
    "                else:\n",
    "                    print(f\"  {subdir}/: NOT FOUND\")\n",
    "    else:\n",
    "        print(\"Failed to filter CSV file\")\n",
    "        raise Exception(\"CSV filtering failed\")\n",
    "else:\n",
    "    print(f\"CSV file {CSV_FILE} not found. Make sure it exists in the current directory.\")\n",
    "    raise Exception(\"CSV file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YBIT5tWikFZD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBIT5tWikFZD",
    "outputId": "11ab6bc7-056b-4cdd-ed17-8b63af1711df"
   },
   "outputs": [],
   "source": [
    "# HOTFIX: Reorganize video files that are in video/ subfolders\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "def fix_video_structure(dataset_root):\n",
    "    \"\"\"Fix video files that are sitting in video/ subfolders\"\"\"\n",
    "    extracted_tmp = Path(dataset_root) / \"_extracted_tmp\"\n",
    "    videos_dir = Path(dataset_root) / \"videos\"\n",
    "    if not extracted_tmp.exists():\n",
    "        print(\"No _extracted_tmp found, skipping video fix\")\n",
    "        return\n",
    "    # Find all Videos_* directories\n",
    "    for item in extracted_tmp.rglob(\"*\"):\n",
    "        if item.is_dir() and item.name.lower().startswith(\"videos_\"):\n",
    "            print(f\"Found Videos directory: {item}\")\n",
    "            # Check for video subfolder\n",
    "            video_subdir = item / \"video\"\n",
    "            if video_subdir.exists() and video_subdir.is_dir():\n",
    "                print(f\"  Found video subfolder: {video_subdir}\")\n",
    "                # Copy all mp4 files from video subfolder\n",
    "                for mp4_file in video_subdir.glob(\"*.mp4\"):\n",
    "                    dst = videos_dir / mp4_file.name\n",
    "                    if not dst.exists():\n",
    "                        videos_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        shutil.move(str(mp4_file), str(dst))\n",
    "                        print(f\"    Moved: {mp4_file.name}\")\n",
    "                    else:\n",
    "                        print(f\"    Skip (exists): {mp4_file.name}\")\n",
    "# Run the hotfix\n",
    "fix_video_structure(DATASET_ROOT)\n",
    "print(\"Video structure hotfix complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4t9lqbzgpe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4t9lqbzgpe",
    "outputId": "eb80f543-6b4d-4274-abe8-5cdecc7fbd82"
   },
   "outputs": [],
   "source": "# Step 2: Modern Segment-First Pipeline with TransNetV2\nprint(\"Step 2: Segment-first pipeline with TransNetV2 + GPU-accelerated indexing\")\nstart_time = time.time()\n\n# Check for videos directory\nvideos_dir = pathlib.Path(DATASET_ROOT) / \"videos\"\nif not videos_dir.exists():\n    print(f\"Warning: {videos_dir} not found, using dataset root as video directory\")\n    videos_dir = pathlib.Path(DATASET_ROOT)\n\n# Check if GPU is available\nuse_gpu = False\ntry:\n    import torch\n    use_gpu = torch.cuda.is_available()\n    if use_gpu:\n        print(\"GPU detected - using TransNetV2 + GPU-accelerated FAISS\")\n    else:\n        print(\"No GPU detected - using OpenCV segmentation + CPU FAISS\")\nexcept Exception:\n    print(\"Could not detect GPU - using CPU processing\")\n\n# Step 2a: Video Segmentation with TransNetV2\nprint(\"\\nStep 2a: Video segmentation...\")\ncmd = [\n    'python', 'scripts/segment_videos.py',\n    '--dataset_root', DATASET_ROOT,\n    '--videos'\n] + VIDEOS + [\n    '--artifact_dir', './artifacts'\n]\n\n# Add TransNetV2 flag if GPU available\nif use_gpu:\n    cmd.append('--use_transnetv2')\n    print(\"Using TransNetV2 for deep learning shot boundary detection\")\nelse:\n    print(\"Using OpenCV fallback for shot boundary detection\")\n\nprint(f\"Segmentation command: {' '.join(cmd)}\")\n\n# Run segmentation\nprocess = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                         universal_newlines=True, bufsize=1)\n\nwhile True:\n    output = process.stdout.readline()\n    if output == '' and process.poll() is not None:\n        break\n    if output:\n        print(f\"[SEGMENT] {output.strip()}\")\n        sys.stdout.flush()\n\nif process.poll() != 0:\n    print(\"Video segmentation failed!\")\n    raise Exception(\"Segmentation failed\")\n\nprint(\"✅ Video segmentation completed\")\n\n# Step 2b: Build Search Index\nprint(\"\\nStep 2b: Building search index...\")\ncmd = [\n    'python', 'scripts/index.py',\n    '--dataset_root', DATASET_ROOT,\n    '--videos'\n] + VIDEOS + [\n    '--segments', './artifacts/segments.parquet'\n]\n\nprint(f\"Indexing command: {' '.join(cmd)}\")\n\n# Run indexing\nprocess = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                         universal_newlines=True, bufsize=1)\n\nwhile True:\n    output = process.stdout.readline()\n    if output == '' and process.poll() is not None:\n        break\n    if output:\n        print(f\"[INDEX] {output.strip()}\")\n        sys.stdout.flush()\n\nif process.poll() != 0:\n    print(\"Index building failed!\")\n    raise Exception(\"Indexing failed\")\n\nprint(\"✅ Search index completed\")\n\n# Step 2c: Build Text Corpus (with ASR if available)\nprint(\"\\nStep 2c: Building text corpus...\")\ncmd = [\n    'python', 'scripts/build_text.py',\n    '--dataset_root', DATASET_ROOT,\n    '--videos'\n] + VIDEOS + [\n    '--artifact_dir', './artifacts',\n    '--segments', './artifacts/segments.parquet'\n]\n\nprint(f\"Text corpus command: {' '.join(cmd)}\")\n\n# Run text corpus building\nprocess = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n                         universal_newlines=True, bufsize=1)\n\nwhile True:\n    output = process.stdout.readline()\n    if output == '' and process.poll() is not None:\n        break\n    if output:\n        print(f\"[TEXT] {output.strip()}\")\n        sys.stdout.flush()\n\nif process.poll() != 0:\n    print(\"Text corpus building failed!\")\n    raise Exception(\"Text corpus building failed\")\n\nprint(\"✅ Text corpus completed\")\n\nelapsed = time.time() - start_time\nprint(f\"\\n🚀 Modern segment-first pipeline completed in {elapsed:.1f} seconds\")\nprint(\"📈 Key advantages:\")\nprint(\"  - TransNetV2 deep learning shot detection\")\nprint(\"  - GPU-accelerated FAISS indexing\")  \nprint(\"  - Segment-level retrieval precision\")\nprint(\"  - ASR corpus integration ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ppVQBDzKkCbW",
   "metadata": {
    "id": "ppVQBDzKkCbW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4rbcsjbwmfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rbcsjbwmfa",
    "outputId": "0f7122ef-0e87-4c1f-a32b-15006da0ec53"
   },
   "outputs": [],
   "source": "# Step 3: Check Pipeline Results\nprint(\"Step 3: Checking segment-first pipeline results\")\n\n# Check generated artifacts\nartifacts_dir = pathlib.Path('./artifacts')\nif artifacts_dir.exists():\n    artifact_files = list(artifacts_dir.glob('*'))\n    print(f\"✅ Generated {len(artifact_files)} artifact files:\")\n    for artifact in sorted(artifact_files):\n        if artifact.is_file():\n            size = artifact.stat().st_size\n            print(f\"  - {artifact.name} ({size:,} bytes)\")\n        elif artifact.is_dir():\n            file_count = len(list(artifact.glob('*')))\n            print(f\"  - {artifact.name}/ ({file_count} files)\")\n\n    # Check for expected artifacts from segment-first pipeline\n    expected_artifacts = ['segments.parquet', 'index.faiss', 'mapping.parquet', 'text_corpus.jsonl']\n    missing_artifacts = []\n    for expected in expected_artifacts:\n        if not (artifacts_dir / expected).exists():\n            missing_artifacts.append(expected)\n    \n    if missing_artifacts:\n        print(f\"\\n⚠️  Missing expected artifacts: {missing_artifacts}\")\n    else:\n        print(f\"\\n🚀 Segment-first pipeline completed! All artifacts ready in ./artifacts/\")\n        print(\"📈 Pipeline components:\")\n        print(\"  - segments.parquet: Video segments with TransNetV2/OpenCV detection\")\n        print(\"  - index.faiss: GPU-accelerated visual search index\") \n        print(\"  - mapping.parquet: Frame-to-video mapping\")\n        print(\"  - text_corpus.jsonl: Text search corpus (with ASR if available)\")\nelse:\n    print(\"❌ No artifacts directory found - pipeline may have failed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fd9fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d8fd9fa",
    "outputId": "1592c006-cf42-4604-9518-43b9cf09644b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data and training reranker...\n",
      "Step 4a: Generating training data from competition metadata...\n",
      "Command: python scripts/create_training_data.py --dataset_root /content/aic2025 --output data/train.jsonl --num_examples 100\n",
      "Training data generated successfully!\n",
      "Using metadata from:\n",
      "  Media info: /content/aic2025/media_info\n",
      "  Keyframes: /content/aic2025/map_keyframes\n",
      "Found 873 videos with complete metadata\n",
      "Sampling 20 videos for training data generation\n",
      "Generated 80 training examples\n",
      "Training data saved to data/train.jsonl\n",
      "\n",
      "✅ Training data creation successful!\n",
      "Next step: Train the reranker with:\n",
      "python src/training/train_reranker.py --index_dir ./artifacts --train_jsonl data/train.jsonl\n",
      "\n",
      "\n",
      "Step 4b: Training reranker model...\n",
      "Command: python src/training/train_reranker.py --index_dir ./artifacts --train_jsonl data/train.jsonl\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "n_iter_i = _check_optimize_result(\n",
      "[OK] trained reranker on 48000 samples; saved → artifacts/reranker.joblib\n",
      "\n",
      "Reranker training completed successfully in 225.5s!\n",
      "Model saved: artifacts/reranker.joblib (1,037 bytes)\n",
      "✅ Reranker training step completed!\n",
      "🎯 Using trained reranker model for enhanced search results\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate Training Data and Train Reranker Model\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def create_and_train_reranker():\n",
    "    \"\"\"Generate training data from metadata and train reranker model\"\"\"\n",
    "    try:\n",
    "        # Step 1: Generate training data from competition metadata\n",
    "        print(\"Step 4a: Generating training data from competition metadata...\")\n",
    "\n",
    "        cmd = [\n",
    "            'python', 'scripts/create_training_data.py',\n",
    "            '--dataset_root', DATASET_ROOT,\n",
    "            '--output', 'data/train.jsonl',\n",
    "            '--num_examples', '100'\n",
    "        ]\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Training data generation failed: {result.stderr}\")\n",
    "            print(\"Will proceed with fusion baseline (no reranker training)\")\n",
    "            return True  # Don't fail pipeline, just use baseline\n",
    "\n",
    "        print(\"Training data generated successfully!\")\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "\n",
    "        # Step 2: Train the reranker model\n",
    "        print(\"\\nStep 4b: Training reranker model...\")\n",
    "\n",
    "        cmd = [\n",
    "            'python', 'src/training/train_reranker.py',\n",
    "            '--index_dir', './artifacts',\n",
    "            '--train_jsonl', 'data/train.jsonl'\n",
    "        ]\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run with real-time output for progress tracking\n",
    "        import sys\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                                 universal_newlines=True, bufsize=1)\n",
    "\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        return_code = process.poll()\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if return_code == 0:\n",
    "            print(f\"\\nReranker training completed successfully in {elapsed_time:.1f}s!\")\n",
    "\n",
    "            # Check if model was created\n",
    "            model_file = pathlib.Path('./artifacts/reranker.joblib')\n",
    "            if model_file.exists():\n",
    "                model_size = model_file.stat().st_size\n",
    "                print(f\"Model saved: {model_file} ({model_size:,} bytes)\")\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Reranker training failed after {elapsed_time:.1f}s!\")\n",
    "            print(\"Will use fusion baseline instead\")\n",
    "            return True  # Don't fail pipeline, baseline still works\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        print(\"Will use fusion baseline instead\")\n",
    "        return True  # Don't fail pipeline\n",
    "\n",
    "# Run training data creation and reranker training\n",
    "print(\"Creating training data and training reranker...\")\n",
    "train_success = create_and_train_reranker()\n",
    "\n",
    "if train_success:\n",
    "    print(\"✅ Reranker training step completed!\")\n",
    "\n",
    "    # Check what we ended up with\n",
    "    model_file = pathlib.Path('./artifacts/reranker.joblib')\n",
    "    if model_file.exists():\n",
    "        print(\"🎯 Using trained reranker model for enhanced search results\")\n",
    "    else:\n",
    "        print(\"📊 Using fusion baseline (RRF) for reliable search results\")\n",
    "else:\n",
    "    print(\"❌ Training failed, but pipeline can continue with fusion baseline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545a47c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f545a47c",
    "outputId": "e2554416-95d9-475d-ac5f-d10c097a4f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline assembly and testing...\n",
      "Assembling minimal pipeline directory...\n",
      "Command: python scripts/prepare_pipeline_dir.py --outdir my_pipeline --artifact_dir ./artifacts --include_model --force\n",
      "Pipeline directory assembled successfully!\n",
      "\n",
      "Pipeline directory contents (my_pipeline/):\n",
      "  FILE README_RUN.md (324 bytes)\n",
      "  DIR  artifacts/ (4 files)\n",
      "  FILE artifacts/index.faiss (34,603,053 bytes)\n",
      "  FILE artifacts/mapping.parquet (384,309 bytes)\n",
      "  FILE artifacts/reranker.joblib (1,037 bytes)\n",
      "  FILE artifacts/text_corpus.jsonl (79,541,830 bytes)\n",
      "  FILE config.py (2,015 bytes)\n",
      "  DIR  src/ (1 files)\n",
      "  DIR  src/retrieval/ (1 files)\n",
      "  FILE src/retrieval/use.py (13,280 bytes)\n",
      "  DIR  submissions/ (0 files)\n",
      "  FILE utils.py (1,124 bytes)\n",
      "\n",
      "Testing pipeline with query: 'a person opening a laptop'\n",
      "Command: python src/retrieval/use.py --query a person opening a laptop\n",
      "[OK] wrote 100 lines → submissions/kis_a-person-opening-a-laptop.csv\n",
      "\n",
      "✅ Test query completed successfully in 14.2s!\n",
      "\n",
      "Generated 1 result file(s):\n",
      "  - submissions/kis_a-person-opening-a-laptop.csv\n",
      "    Sample results (first 3 lines):\n",
      "    1: L22_V014,22301\n",
      "    2: L22_V003,17019\n",
      "    3: L22_V018,19992\n",
      "    Total results: 100\n",
      "\n",
      "🎉 Pipeline assembled and tested successfully!\n",
      "Ready-to-deploy pipeline is in: my_pipeline/\n",
      "\n",
      "Next steps:\n",
      "  1. Upload my_pipeline/ to your deployment environment\n",
      "  2. Run queries using: python src/retrieval/use.py --query 'your search'\n",
      "  3. Find results in submissions/ folder\n",
      "\n",
      "Development Summary:\n",
      "  Pipeline directory: my_pipeline/\n",
      "  Test query: 'a person opening a laptop'\n",
      "  Results location: my_pipeline/submissions/\n",
      "  Model: Trained reranker included (1,037 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Assemble Pipeline and Test Query\n",
    "import os\n",
    "import subprocess\n",
    "import pathlib\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "PIPELINE_DIR = 'my_pipeline'\n",
    "TEST_QUERY = 'a person opening a laptop'\n",
    "\n",
    "def assemble_and_test_pipeline():\n",
    "    \"\"\"Assemble minimal pipeline directory and run a test query\"\"\"\n",
    "    try:\n",
    "        # Step 1: Prepare pipeline directory\n",
    "        print(\"Assembling minimal pipeline directory...\")\n",
    "\n",
    "        cmd = [\n",
    "            'python', 'scripts/prepare_pipeline_dir.py',\n",
    "            '--outdir', PIPELINE_DIR,\n",
    "            '--artifact_dir', './artifacts',\n",
    "            '--include_model',\n",
    "            '--force'\n",
    "        ]\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Pipeline assembly failed: {result.stderr}\")\n",
    "            return False\n",
    "\n",
    "        print(\"Pipeline directory assembled successfully!\")\n",
    "\n",
    "        # Show pipeline contents\n",
    "        pipeline_path = pathlib.Path(PIPELINE_DIR)\n",
    "        if pipeline_path.exists():\n",
    "            print(f\"\\nPipeline directory contents ({PIPELINE_DIR}/):\")\n",
    "            for item in sorted(pipeline_path.rglob('*')):\n",
    "                if item.is_file():\n",
    "                    rel_path = item.relative_to(pipeline_path)\n",
    "                    size = item.stat().st_size\n",
    "                    print(f\"  FILE {rel_path} ({size:,} bytes)\")\n",
    "                elif item.is_dir() and item != pipeline_path:\n",
    "                    rel_path = item.relative_to(pipeline_path)\n",
    "                    file_count = len(list(item.glob('*')))\n",
    "                    print(f\"  DIR  {rel_path}/ ({file_count} files)\")\n",
    "\n",
    "        # Step 2: Test the pipeline\n",
    "        print(f\"\\nTesting pipeline with query: '{TEST_QUERY}'\")\n",
    "\n",
    "        # Change to pipeline directory for testing\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(PIPELINE_DIR)\n",
    "\n",
    "        try:\n",
    "            cmd = ['python', 'src/retrieval/use.py', '--query', TEST_QUERY]\n",
    "            print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run with real-time output to see any import errors immediately\n",
    "            import sys\n",
    "            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                                     universal_newlines=True, bufsize=1)\n",
    "\n",
    "            output_lines = []\n",
    "            while True:\n",
    "                output = process.stdout.readline()\n",
    "                if output == '' and process.poll() is not None:\n",
    "                    break\n",
    "                if output:\n",
    "                    output_lines.append(output.strip())\n",
    "                    print(output.strip())\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            return_code = process.poll()\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            if return_code == 0:\n",
    "                print(f\"\\n✅ Test query completed successfully in {elapsed_time:.1f}s!\")\n",
    "\n",
    "                # Show results\n",
    "                submissions_dir = pathlib.Path('submissions')\n",
    "                if submissions_dir.exists():\n",
    "                    csv_files = list(submissions_dir.glob('*.csv'))\n",
    "                    if csv_files:\n",
    "                        print(f\"\\nGenerated {len(csv_files)} result file(s):\")\n",
    "                        for csv_file in csv_files:\n",
    "                            print(f\"  - {csv_file}\")\n",
    "                            # Show first few lines\n",
    "                            try:\n",
    "                                with open(csv_file, 'r') as f:\n",
    "                                    lines = f.readlines()[:3]\n",
    "                                    print(f\"    Sample results (first 3 lines):\")\n",
    "                                    for i, line in enumerate(lines, 1):\n",
    "                                        print(f\"    {i}: {line.strip()}\")\n",
    "                                # Count total results\n",
    "                                with open(csv_file, 'r') as f:\n",
    "                                    total_lines = sum(1 for _ in f)\n",
    "                                print(f\"    Total results: {total_lines}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"    (Could not read file: {e})\")\n",
    "\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\n❌ Test query failed after {elapsed_time:.1f}s!\")\n",
    "\n",
    "                # Show detailed error information\n",
    "                error_found = False\n",
    "                for line in output_lines:\n",
    "                    if \"Error\" in line or \"Traceback\" in line or \"ImportError\" in line:\n",
    "                        error_found = True\n",
    "                        print(f\"Error: {line}\")\n",
    "\n",
    "                if not error_found and output_lines:\n",
    "                    print(\"Last few lines of output:\")\n",
    "                    for line in output_lines[-5:]:\n",
    "                        print(f\"  {line}\")\n",
    "\n",
    "                return False\n",
    "\n",
    "        finally:\n",
    "            # Return to original directory\n",
    "            os.chdir(original_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Assembly/test error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run assembly and test\n",
    "print(\"Starting pipeline assembly and testing...\")\n",
    "success = assemble_and_test_pipeline()\n",
    "\n",
    "if success:\n",
    "    print(f\"\\n🎉 Pipeline assembled and tested successfully!\")\n",
    "    print(f\"Ready-to-deploy pipeline is in: {PIPELINE_DIR}/\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(f\"  1. Upload {PIPELINE_DIR}/ to your deployment environment\")\n",
    "    print(\"  2. Run queries using: python src/retrieval/use.py --query 'your search'\")\n",
    "    print(\"  3. Find results in submissions/ folder\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Pipeline assembly or testing failed. Check errors above.\")\n",
    "\n",
    "# Show final summary\n",
    "print(f\"\\nDevelopment Summary:\")\n",
    "print(f\"  Pipeline directory: {PIPELINE_DIR}/\")\n",
    "print(f\"  Test query: '{TEST_QUERY}'\")\n",
    "print(f\"  Results location: {PIPELINE_DIR}/submissions/\")\n",
    "\n",
    "# Check for trained model\n",
    "model_file = pathlib.Path(f'{PIPELINE_DIR}/artifacts/reranker.joblib')\n",
    "if model_file.exists():\n",
    "    print(f\"  Model: Trained reranker included ({model_file.stat().st_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"  Model: Using fusion baseline (RRF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d658ab",
   "metadata": {
    "id": "57d658ab"
   },
   "source": [
    "## Official Evaluation\n",
    "Provide your ground truth JSON path and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3688a",
   "metadata": {
    "id": "a6f3688a"
   },
   "outputs": [],
   "source": [
    "# Configure evaluation\n",
    "GT_PATH = 'ground_truth.json'   # update path (e.g., /content/drive/MyDrive/gt.json)\n",
    "TASK_EVAL = 'kis'               # 'kis' or 'vqa' or 'trake'\n",
    "NORMALIZE_ANS = False           # True to casefold VQA answers\n",
    "\n",
    "import subprocess\n",
    "cmd = ['python', 'eval/evaluate.py', '--gt', GT_PATH, '--pred_dir', 'submissions', '--task', TASK_EVAL]\n",
    "if NORMALIZE_ANS:\n",
    "    cmd.append('--normalize_answer')\n",
    "print('Evaluating:', ' '.join(cmd))\n",
    "res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(res.stdout)\n",
    "if res.returncode:\n",
    "    print(res.stderr)\n",
    "    raise SystemExit(res.returncode)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
