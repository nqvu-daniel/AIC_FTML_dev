{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca765bd",
   "metadata": {},
   "source": [
    "## System Check & Setup\n",
    "\n",
    "Run this cell first to check your Colab environment and system capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2adv54bvy",
   "metadata": {},
   "source": [
    "# AIC 2024/2025 Retrieval – Automated Google Colab Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nqvu-daniel/AIC_FTML_dev/blob/main/notebooks/colab_pipeline.ipynb)\n",
    "\n",
    "**Quick Start:**\n",
    "1. **Enable GPU**: Runtime → Change runtime type → Hardware accelerator: T4/L4/A100 (recommended)\n",
    "2. **Run Setup**: Execute the \"Setup\" cell below to automatically clone repo and install dependencies\n",
    "3. **Choose Your Path**:\n",
    "   - Host Inference (recommended): Use pre-built artifacts to run queries instantly\n",
    "   - Development Pipeline: Build your own artifacts from scratch (requires dataset)\n",
    "\n",
    "**File Downloads**: Results are saved to `/content/AIC_FTML_dev/submissions/` - you can download them from Colab's file browser.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Usage Modes\n",
    "\n",
    "### 1. Host Inference (Recommended - Fast)\n",
    "- No dataset required\n",
    "- Uses pre-built artifacts and models\n",
    "- Ready in ~2 minutes\n",
    "- Perfect for running queries and getting CSV results\n",
    "\n",
    "### 2. Development Pipeline (Advanced - Slow)\n",
    "- Downloads full dataset (alot~GBs)\n",
    "- Builds search index from scratch + custom reranker models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi || true\n",
    "!python --version\n",
    "import sys, os, pathlib\n",
    "print('CWD:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5e6ae",
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Clone repo and install dependencies automatically (GPU-ready)\nimport os\nimport pathlib\nimport subprocess\nimport sys\n\nREPO_URL = 'https://github.com/nqvu-daniel/AIC_FTML_dev.git'\nREPO_NAME = 'AIC_FTML_dev'\n\ndef setup_repository():\n    \"\"\"Automatically clone repository and setup environment\"\"\"\n    try:\n        # Check if repo already exists\n        if pathlib.Path(REPO_NAME).exists():\n            print(f\"Repository '{REPO_NAME}' already exists\")\n            os.chdir(REPO_NAME)\n        else:\n            print(f\"Cloning repository from {REPO_URL}\")\n            subprocess.run(['git', 'clone', REPO_URL], check=True)\n            os.chdir(REPO_NAME)\n            print(\"Repository cloned successfully\")\n        \n        # Install dependencies\n        print(\"Installing dependencies...\")\n        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', 'requirements.txt'], check=True)\n        \n        # Install FAISS based on CUDA availability\n        try:\n            import torch\n            if torch.cuda.is_available():\n                print(\"GPU detected, installing faiss-gpu-cu12...\")\n                subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-gpu-cu12'], check=True)\n                print('Installed faiss-gpu-cu12 (CUDA 12 compatible)')\n            else:\n                print(\"No GPU detected, installing faiss-cpu...\")\n                subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-cpu'], check=True)\n                print('Installed faiss-cpu')\n        except Exception as e:\n            print(f'FAISS install error: {e}')\n            # Fallback to CPU version\n            subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-cpu'], check=True)\n            print('Fallback: Installed faiss-cpu')\n        \n        # Add to Python path\n        if '.' not in sys.path:\n            sys.path.append('.')\n            \n        print(\"Setup complete! Ready to run AIC FTML pipeline\")\n        print(f\"Current directory: {os.getcwd()}\")\n        \n        return True\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"Error during setup: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return False\n\n# Run setup\nif setup_repository():\n    print(\"\\nYou can now proceed with the pipeline!\")\nelse:\n    print(\"\\nSetup failed. Please check the errors above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host Inference – One-shot\n",
    "Provide `ARTIFACTS_BUNDLE_URL` and/or `RERANKER_MODEL_URL` if not already present in `./artifacts`.\n",
    "This writes a Top-100 CSV into `submissions/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74133bd3",
   "metadata": {},
   "outputs": [],
   "source": "# Host Inference - Automated Setup and Query Execution\nimport os\nimport subprocess\nimport pathlib\n\n# Configuration - Update these URLs with your hosted models\nQUERY = 'a person opening a laptop'  # Change this to your search query\nQUERY_ID = 'q1'  # Official query id for filename submissions/{query_id}.csv\nTASK = 'kis'     # 'kis' or 'vqa'\nANSWER = ''      # Required if TASK='vqa'\nARTIFACTS_BUNDLE_URL = ''  # e.g., 'https://your-host.com/artifacts_bundle.tar.gz'\nRERANKER_MODEL_URL = ''    # e.g., 'https://your-host.com/reranker.joblib'\n\ndef run_inference_query(query, bundle_url='', model_url='', query_id='', task='kis', answer=''):\n    \"\"\"Run inference with automatic artifact download if needed\"\"\"\n    try:\n        # Ensure we're in the right directory\n        if not pathlib.Path('src/retrieval/use.py').exists():\n            print('Missing use.py script. Make sure setup completed successfully.')\n            return False\n            \n        # Build command\n        cmd = ['python', 'src/retrieval/use.py', '--query', query, '--task', task]\n        if query_id:\n            cmd.extend(['--query_id', query_id])\n        if task == 'vqa':\n            if not answer:\n                print('For TASK=vqa you must set ANSWER.')\n                return False\n            cmd.extend(['--answer', answer])\n            \n        if bundle_url:\n            cmd.extend(['--bundle_url', bundle_url])\n            print(f'Will download artifacts bundle from: {bundle_url}')\n            \n        if model_url:\n            cmd.extend(['--model_url', model_url])\n            print(f'Will download reranker model from: {model_url}')\n        \n        # Create submissions directory if it doesn't exist\n        os.makedirs('submissions', exist_ok=True)\n        \n        print(f\"Running query: '{query}' (task={task}, qid={query_id})\")\n        print('Command:', ' '.join(cmd))\n        \n        # Execute the command\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        \n        if result.returncode == 0:\n            print('Query execution successful!')\n            if result.stdout:\n                print('Output:\\n' + result.stdout)\n            \n            # List generated files\n            submissions_dir = pathlib.Path('submissions')\n            if submissions_dir.exists():\n                csv_files = list(submissions_dir.glob('*.csv'))\n                if csv_files:\n                    print('\\nGenerated ' + str(len(csv_files)) + ' result file(s):')\n                    for csv_file in csv_files:\n                        print(f'  - {csv_file}')\n                        # Show first few lines of the CSV\n                        try:\n                            with open(csv_file, 'r') as f:\n                                lines = f.readlines()[:5]\n                                print('    Preview (first 5 lines):')\n                                for i, line in enumerate(lines, 1):\n                                    print(f'    {i}: {line.strip()}')\n                        except Exception as e:\n                            print(f'    (Could not preview: {e})')\n            return True\n        else:\n            print('Query execution failed!')\n            print('Error output:\\n' + result.stderr)\n            return False\n            \n    except Exception as e:\n        print(f'Error running inference: {e}')\n        return False\n\n# Run the inference\nprint('Starting AIC FTML Host Inference...')\nsuccess = run_inference_query(QUERY, ARTIFACTS_BUNDLE_URL, RERANKER_MODEL_URL, QUERY_ID, TASK, ANSWER)\n\nif success:\n    print('\\nInference completed! Check the submissions/ folder for results.')\nelse:\n    print('\\nInference failed. Check the error messages above.')"
  },
  {
   "cell_type": "markdown",
   "id": "5eefaa0b",
   "metadata": {},
   "source": [
    "## Dev Pipeline – Build Artifacts (Optional)\n",
    "Downloads dataset archives using `AIC_2025_dataset_download_link.csv`, builds index/corpus, optionally trains reranker, and assembles `my_pipeline/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc09538",
   "metadata": {},
   "outputs": [],
   "source": "# Dev Pipeline - Automated Dataset Download and Processing\nimport os\nimport subprocess\nimport pathlib\nimport time\nimport csv\nimport tempfile\n\n# Configuration\nDATASET_ROOT = '/content/aic2025'\n# TEST_MODE = True  # Uncomment to enable test mode (only downloads L21-L24)\nVIDEOS = ['L21', 'L22', 'L23', 'L24', 'L25', 'L26', 'L27', 'L28', 'L29', 'L30']  # adjust if needed\nCSV_FILE = 'AIC_2025_dataset_download_link.csv'  # Update path if different\n\n# Apply test mode if enabled\ntry:\n    if TEST_MODE:\n        VIDEOS = ['L21', 'L22', 'L23', 'L24']\n        print(\"TEST MODE ENABLED: Only downloading L21-L24\")\nexcept NameError:\n    pass  # TEST_MODE not defined, use full video list\n\ndef filter_csv_for_videos(csv_path, video_list, output_path):\n    \"\"\"Filter the CSV file to only include entries for specified videos\"\"\"\n    if not pathlib.Path(csv_path).exists():\n        return False\n    \n    filtered_rows = []\n    with open(csv_path, 'r', encoding='utf-8') as f:\n        reader = csv.reader(f)\n        header = next(reader, None)\n        if header:\n            filtered_rows.append(header)\n        \n        for row in reader:\n            if not row:\n                continue\n            # Check if any of our target videos appear in the filename\n            filename = row[-2].strip() if len(row) >= 2 else \"\"\n            should_include = any(vid in filename.upper() for vid in video_list)\n            if should_include:\n                filtered_rows.append(row)\n    \n    # Write filtered CSV\n    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerows(filtered_rows)\n    \n    print(f\"Filtered CSV: {len(filtered_rows)-1} entries for videos {video_list}\")\n    return True\n\ndef run_dev_pipeline():\n    \"\"\"Run the complete development pipeline with progress tracking\"\"\"\n    steps = [\n        \"Download dataset\",\n        \"Build search index\", \n        \"Build text corpus\"\n    ]\n    \n    # Check if GPU is available for flat indexing\n    use_gpu = False\n    try:\n        import torch\n        use_gpu = torch.cuda.is_available()\n        if use_gpu:\n            print(\"GPU detected - will build flat index for GPU acceleration\")\n        else:\n            print(\"No GPU detected - building HNSW index for CPU\")\n    except Exception:\n        print(\"Could not detect GPU - building HNSW index for CPU\")\n    \n    try:\n        # Step 1: Download dataset\n        print(f\"Step 1/3: {steps[0]}\")\n        if pathlib.Path(CSV_FILE).exists():\n            # Create filtered CSV for our target videos\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp_csv:\n                filtered_csv_path = tmp_csv.name\n            \n            if filter_csv_for_videos(CSV_FILE, VIDEOS, filtered_csv_path):\n                cmd = [\n                    'python', 'scripts/dataset_downloader.py', \n                    '--dataset_root', DATASET_ROOT,\n                    '--csv', filtered_csv_path,\n                    '--skip-existing'\n                ]\n                print(f\"Command: {' '.join(cmd)}\")\n                result = subprocess.run(cmd, capture_output=True, text=True)\n                \n                # Clean up temp file\n                try:\n                    os.unlink(filtered_csv_path)\n                except:\n                    pass\n                \n                if result.returncode != 0:\n                    print(f\"Dataset download failed: {result.stderr}\")\n                    return False\n                print(\"Dataset download completed\")\n            else:\n                print(\"Failed to filter CSV file\")\n                return False\n        else:\n            print(f\"CSV file {CSV_FILE} not found. Skipping dataset download.\")\n        \n        # Step 2: Build index\n        print(f\"\\nStep 2/3: {steps[1]}\")\n        cmd = [\n            'python', 'scripts/index.py',\n            '--dataset_root', DATASET_ROOT,\n            '--videos'\n        ] + VIDEOS\n        \n        # Add --flat flag for GPU compatibility\n        if use_gpu:\n            cmd.append('--flat')\n            print(\"Building flat index for GPU acceleration\")\n        \n        print(f\"Command: {' '.join(cmd)}\")\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(f\"Index building failed: {result.stderr}\")\n            return False\n        print(\"Search index built successfully\")\n        \n        # Step 3: Build text corpus\n        print(f\"\\nStep 3/3: {steps[2]}\")\n        cmd = [\n            'python', 'scripts/build_text.py',\n            '--dataset_root', DATASET_ROOT,\n            '--videos'\n        ] + VIDEOS\n        print(f\"Command: {' '.join(cmd)}\")\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(f\"Text corpus building failed: {result.stderr}\")\n            return False\n        print(\"Text corpus built successfully\")\n        \n        # Check artifacts\n        artifacts_dir = pathlib.Path('./artifacts')\n        if artifacts_dir.exists():\n            artifact_files = list(artifacts_dir.glob('*'))\n            print(f\"\\nGenerated {len(artifact_files)} artifact files:\")\n            for artifact in artifact_files[:10]:  # Show first 10\n                size = artifact.stat().st_size if artifact.is_file() else 0\n                print(f\"  - {artifact.name} ({size:,} bytes)\")\n            if len(artifact_files) > 10:\n                print(f\"  ... and {len(artifact_files) - 10} more files\")\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Pipeline error: {e}\")\n        return False\n\n# Run the development pipeline\nprint(\"Starting AIC FTML Development Pipeline...\")\nstart_time = time.time()\n\nsuccess = run_dev_pipeline()\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nPipeline completed in {elapsed_time:.1f} seconds\")\n\nif success:\n    print(\"Development pipeline completed successfully!\")\n    print(\"Artifacts are ready in ./artifacts/\")\nelse:\n    print(\"Pipeline failed. Check error messages above.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8fd9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Train Reranker Model\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "# Configuration\n",
    "TRAIN_JSONL_PATHS = ['data/train.jsonl', 'data/train_dev.jsonl', 'train.jsonl']  # Check multiple possible locations\n",
    "MODEL_OUTPUT_PATH = './artifacts/reranker.joblib'\n",
    "\n",
    "def train_reranker():\n",
    "    \"\"\"Train reranker model if training data is available\"\"\"\n",
    "    try:\n",
    "        # Find training data\n",
    "        train_file = None\n",
    "        for path in TRAIN_JSONL_PATHS:\n",
    "            if pathlib.Path(path).exists():\n",
    "                train_file = path\n",
    "                break\n",
    "        \n",
    "        if not train_file:\n",
    "            print(\"No training JSONL found in:\")\n",
    "            for path in TRAIN_JSONL_PATHS:\n",
    "                print(f\"  - {path}\")\n",
    "            print(\"Skipping reranker training (will use fusion baseline)\")\n",
    "            return True\n",
    "        \n",
    "        print(f\"Found training data: {train_file}\")\n",
    "        \n",
    "        # Count training samples\n",
    "        try:\n",
    "            with open(train_file, 'r') as f:\n",
    "                num_samples = sum(1 for _ in f)\n",
    "            print(f\"Training samples: {num_samples}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not count samples: {e}\")\n",
    "        \n",
    "        # Check if model already exists\n",
    "        if pathlib.Path(MODEL_OUTPUT_PATH).exists():\n",
    "            response = input(f\"Model already exists at {MODEL_OUTPUT_PATH}. Retrain? (y/N): \")\n",
    "            if response.lower() != 'y':\n",
    "                print(\"Skipping reranker training\")\n",
    "                return True\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"Training reranker model...\")\n",
    "        cmd = [\n",
    "            'python', 'src/training/train_reranker.py',\n",
    "            '--index_dir', './artifacts',\n",
    "            '--train_jsonl', train_file\n",
    "        ]\n",
    "        print(f\"Command: {' '.join(cmd)}\")\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"Reranker training completed successfully!\")\n",
    "            print(f\"Model saved to: {MODEL_OUTPUT_PATH}\")\n",
    "            \n",
    "            # Show model info\n",
    "            if pathlib.Path(MODEL_OUTPUT_PATH).exists():\n",
    "                model_size = pathlib.Path(MODEL_OUTPUT_PATH).stat().st_size\n",
    "                print(f\"Model size: {model_size:,} bytes\")\n",
    "            \n",
    "            if result.stdout:\n",
    "                print(f\"Training output:\\n{result.stdout}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"Reranker training failed!\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run training\n",
    "print(\"Checking for reranker training...\")\n",
    "train_success = train_reranker()\n",
    "\n",
    "if train_success:\n",
    "    print(\"Reranker training step completed!\")\n",
    "else:\n",
    "    print(\"Reranker training failed, but pipeline can continue with fusion baseline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545a47c",
   "metadata": {},
   "outputs": [],
   "source": "# Assemble Pipeline and Test Query\nimport os\nimport subprocess\nimport pathlib\nimport shutil\nimport time\n\n# Configuration\nPIPELINE_DIR = 'my_pipeline'\nTEST_QUERY = 'a person opening a laptop'\n\ndef assemble_and_test_pipeline():\n    \"\"\"Assemble minimal pipeline directory and run a test query\"\"\"\n    try:\n        # Step 1: Prepare pipeline directory\n        print(\"Assembling minimal pipeline directory...\")\n        \n        cmd = [\n            'python', 'scripts/prepare_pipeline_dir.py',\n            '--outdir', PIPELINE_DIR,\n            '--artifact_dir', './artifacts',\n            '--include_model',\n            '--force'\n        ]\n        print(f\"Command: {' '.join(cmd)}\")\n        \n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            print(f\"Pipeline assembly failed: {result.stderr}\")\n            return False\n        \n        print(\"Pipeline directory assembled successfully!\")\n        \n        # Show pipeline contents\n        pipeline_path = pathlib.Path(PIPELINE_DIR)\n        if pipeline_path.exists():\n            print(f\"\\nPipeline directory contents ({PIPELINE_DIR}/):\")\n            for item in sorted(pipeline_path.rglob('*')):\n                if item.is_file():\n                    rel_path = item.relative_to(pipeline_path)\n                    size = item.stat().st_size\n                    print(f\"  FILE {rel_path} ({size:,} bytes)\")\n                elif item.is_dir() and item != pipeline_path:\n                    rel_path = item.relative_to(pipeline_path)\n                    file_count = len(list(item.glob('*')))\n                    print(f\"  DIR  {rel_path}/ ({file_count} files)\")\n        \n        # Step 2: Test the pipeline\n        print(f\"\\nTesting pipeline with query: '{TEST_QUERY}'\")\n        \n        # Change to pipeline directory\n        original_dir = os.getcwd()\n        os.chdir(PIPELINE_DIR)\n        \n        try:\n            cmd = ['python', 'src/retrieval/use.py', '--query', TEST_QUERY]\n            print(f\"Command: {' '.join(cmd)}\")\n            \n            start_time = time.time()\n            result = subprocess.run(cmd, capture_output=True, text=True)\n            elapsed_time = time.time() - start_time\n            \n            if result.returncode == 0:\n                print(f\"Test query completed successfully in {elapsed_time:.1f}s!\")\n                \n                # Show results\n                submissions_dir = pathlib.Path('submissions')\n                if submissions_dir.exists():\n                    csv_files = list(submissions_dir.glob('*.csv'))\n                    if csv_files:\n                        print(f\"\\nGenerated {len(csv_files)} result file(s):\")\n                        for csv_file in csv_files:\n                            print(f\"  - {csv_file}\")\n                            # Show first few lines\n                            try:\n                                with open(csv_file, 'r') as f:\n                                    lines = f.readlines()[:3]\n                                    print(f\"    Sample results (first 3 lines):\")\n                                    for i, line in enumerate(lines, 1):\n                                        print(f\"    {i}: {line.strip()}\")\n                                # Count total results\n                                with open(csv_file, 'r') as f:\n                                    total_lines = sum(1 for _ in f)\n                                print(f\"    Total results: {total_lines}\")\n                            except Exception as e:\n                                print(f\"    (Could not read file: {e})\")\n                \n                if result.stdout:\n                    print(f\"\\nQuery output:\\n{result.stdout}\")\n                \n                return True\n            else:\n                print(f\"Test query failed after {elapsed_time:.1f}s!\")\n                print(f\"Error: {result.stderr}\")\n                return False\n                \n        finally:\n            # Return to original directory\n            os.chdir(original_dir)\n        \n    except Exception as e:\n        print(f\"Assembly/test error: {e}\")\n        return False\n\n# Run assembly and test\nprint(\"Starting pipeline assembly and testing...\")\nsuccess = assemble_and_test_pipeline()\n\nif success:\n    print(f\"\\nPipeline assembled and tested successfully!\")\n    print(f\"Ready-to-deploy pipeline is in: {PIPELINE_DIR}/\")\n    print(\"\\nNext steps:\")\n    print(f\"  1. Upload {PIPELINE_DIR}/ to your deployment environment\")\n    print(\"  2. Run queries using: python src/retrieval/use.py --query 'your search'\")\n    print(\"  3. Find results in submissions/ folder\")\nelse:\n    print(f\"\\nPipeline assembly or testing failed. Check errors above.\")\n\n# Show final summary\nprint(f\"\\nDevelopment Summary:\")\nprint(f\"  Pipeline directory: {PIPELINE_DIR}/\")\nprint(f\"  Test query: '{TEST_QUERY}'\")\nprint(f\"  Results location: {PIPELINE_DIR}/submissions/\")\nprint(f\"  Model included: {'Yes' if pathlib.Path(f'{PIPELINE_DIR}/artifacts/reranker.joblib').exists() else 'No (using fusion baseline)'}\")"
  },
  {
   "cell_type": "markdown",
   "id": "57d658ab",
   "metadata": {},
   "source": [
    "## Official Evaluation\n",
    "Provide your ground truth JSON path and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure evaluation\n",
    "GT_PATH = 'ground_truth.json'   # update path (e.g., /content/drive/MyDrive/gt.json)\n",
    "TASK_EVAL = 'kis'               # 'kis' or 'vqa' or 'trake'\n",
    "NORMALIZE_ANS = False           # True to casefold VQA answers\n",
    "\n",
    "import subprocess\n",
    "cmd = ['python', 'eval/evaluate.py', '--gt', GT_PATH, '--pred_dir', 'submissions', '--task', TASK_EVAL]\n",
    "if NORMALIZE_ANS:\n",
    "    cmd.append('--normalize_answer')\n",
    "print('Evaluating:', ' '.join(cmd))\n",
    "res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(res.stdout)\n",
    "if res.returncode:\n",
    "    print(res.stderr)\n",
    "    raise SystemExit(res.returncode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}